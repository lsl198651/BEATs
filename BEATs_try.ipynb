{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数冻结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# 加载预训练的模型\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# 冻结模型的部分权重\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 替换或添加自定义的全连接层\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)  # 假设目标任务是10分类\n",
    "\n",
    "# 指定需要训练的参数\n",
    "trainable_params = model.fc.parameters()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(trainable_params, lr=0.001, momentum=0.9)\n",
    "\n",
    "# 在训练循环中只更新需要训练的参数\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存训练日志文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_log_filename = \"train_log.txt\"\n",
    "train_log_filepath = os.path.join(result_dir, train_log_filename)\n",
    "train_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str}\\n\"\n",
    "to_write=train_log_txt_formatter.format(time_str=time.strtime(%Y_%n_%d_%H:%M:%S),\n",
    "                                        epoch=epoch,\n",
    "                                        loss_str=\" \".join([\"{}\".format(loss)]))\n",
    "with open(train_log_filepath,\"a\") as f:\n",
    "    f.write(to_write)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load NPY文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "loadData = np.load(r'E:\\Shilong\\murmur\\03_Classifier\\test_features.npy')\n",
    "\n",
    "print(\"----type----\")\n",
    "print(type(loadData))\n",
    "print(\"----shape----\")\n",
    "print(loadData.shape)\n",
    "print(\"----data----\")\n",
    "print(loadData)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.8100457721031077\n",
      "recall:0.7931778929188256\n",
      "SPE:0.30169157497125965\n",
      "PPV:0.43712076145151696\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "def draw_confusion_matrix(\n",
    "    cm,\n",
    "    label_name,\n",
    "    title=\"Confusion Matrix\",\n",
    "    pdf_save_path=None,\n",
    "    dpi=600,\n",
    "):\n",
    "    row_sums = np.sum(cm, axis=1)  # 计算每行的和\n",
    "    cm = cm.T\n",
    "    plt.imshow(cm.T, cmap=\"Blues\")\n",
    "    # plt.title(title)\n",
    "    plt.xlabel(\"Predict label\")\n",
    "    plt.ylabel(\"Truth label\")\n",
    "    plt.yticks(range(label_name.__len__()), label_name)\n",
    "    plt.xticks(range(label_name.__len__()), label_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.colorbar()\n",
    "    plt.rcParams.update({\"font.size\":15})#此处必须添加此句代码方可改变标题字体大小\n",
    "\n",
    "    for i in range(label_name.__len__()):\n",
    "        for j in range(label_name.__len__()):\n",
    "            color = (1, 1, 1) if i ==0 and  j==0 else (0, 0, 0)  # 对角线字体白色，其他黑色\n",
    "            value = float(format(\"%.4f\" % cm[i, j]))\n",
    "            str_value = \"{}\".format(cm[i, j])\n",
    "            plt.text(\n",
    "                i,\n",
    "                j,\n",
    "                str_value,\n",
    "                verticalalignment=\"center\",\n",
    "                horizontalalignment=\"center\",\n",
    "                color=color,\n",
    "            )\n",
    "\n",
    "    # plt.show()\n",
    "    # if not pdf_save_path is None:\n",
    "    #     if not os.path.exists(pdf_save_path):\n",
    "    #         os.makedirs(pdf_save_path)\n",
    "    #     plt.savefig(\n",
    "    #         pdf_save_path + \"CM\" + \".png\",\n",
    "    #         bbox_inches=\"tight\",\n",
    "    #         dpi=dpi,\n",
    "    #     )\n",
    "        plt.close()\n",
    "    acc=(cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
    "    ppv=cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    recall=cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "    f1=2*recall*ppv/(recall+ppv)\n",
    "\n",
    "    print(\"ACC:\"+str(acc))\n",
    "    print(\"recall:\"+str(recall))\n",
    "    print(\"SPE:\"+str(ppv))\n",
    "    print(\"PPV:\"+str(f1))\n",
    "\n",
    "          \n",
    "\n",
    "draw_confusion_matrix(\n",
    "        np.array([[18338 , 4252],[  479 , 1837]]),\n",
    "        # np.array([[673,22],[65,114 ]]),\n",
    "        [\"Absent\", \"Present\"],\n",
    "        title=\"Confusion Matrix\",\n",
    "        pdf_save_path='./cm_figure',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.8816124109867752\n",
      "recall: 0.7458273724368145\n",
      "pre: 0.4089958158995816\n",
      "f1: 0.5282891403479142\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "cm=np.array([[19235 , 2260],\n",
    " [  533 , 1564]])\n",
    "\n",
    "acc=(cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
    "recall=cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "ppv=cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "f1=2*recall*ppv/(recall+ppv)\n",
    "\n",
    "print(\"ACC: \"+str(acc))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"pre: \"+str(ppv))\n",
    "print(\"f1: \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "y_gt=[]\n",
    "y_pred=[]\n",
    "for index, (labels, imgs) in enumerate(test_loader):\n",
    "    labels_pd = model(imgs)\n",
    "    predict_np = np.argmax(labels_pd.cpu().detach().numpy(), axis=-1)   # array([0,5,1,6,3,...],dtype=int64)\n",
    "    labels_np = labels.numpy()                                          # array([0,5,0,6,2,...],dtype=int64)\n",
    "\t\n",
    "    y_pred.append(labels_np)\n",
    "    y_gt.append(labels_np)\n",
    "    \n",
    "draw_confusion_matrix(label_true=y_gt,\t\t\t# y_gt=[0,5,1,6,3,...]\n",
    "                      label_pred=y_pred,\t    # y_pred=[0,5,1,6,3,...]\n",
    "                      label_name=[\"An\", \"Di\", \"Fe\", \"Ha\", \"Sa\", \"Su\", \"Ne\"],\n",
    "                      normlize=True,\n",
    "                      title=\"Confusion Matrix on Fer2013\",\n",
    "                      pdf_save_path=\"Confusion_Matrix_on_Fer2013.jpg\",\n",
    "                      dpi=300)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 召回率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pred = [0, 1, 0, 1] # 预测的值\n",
    "target = [0, 1, 1, 0] # 真实的值\n",
    "\n",
    "r = recall_score(pred, target)\n",
    "\n",
    "print(r)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不是 0 1 的值，是其他二分类的值，那么就可以通过 labels、pos_label 来指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [3,4]  # 二分类 两个类别的值\n",
    "\n",
    "pred = [3, 4, 3, 4] # 预测的值\n",
    "\n",
    "target = [3, 4, 4, 3] # 真实的值\n",
    "\n",
    "r = recall_score(pred, target , labels = labels , pos_label= 3) # pos_label指定正样本的值是多少\n",
    "\n",
    "print(r)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化\n",
    "YAMNet 还会返回一些可用于可视化的附加信息。我们看一下波形、声谱图和推断的热门类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the waveform.\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(waveform)\n",
    "plt.xlim([0, len(waveform)])\n",
    "\n",
    "# Plot the log-mel spectrogram (returned by the model).\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
    "\n",
    "# Plot and label the model output scores for the top-scoring classes.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "top_n = 10\n",
    "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
    "\n",
    "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
    "# values from the model documentation\n",
    "patch_padding = (0.025 / 2) / 0.01\n",
    "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
    "# Label the top_N classes.\n",
    "yticks = range(0, top_n, 1)\n",
    "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
    "_ = plt.ylim(-0.5 + np.array([top_n, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 筛选样本"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分train-set,val-set,test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_frame, train_samples, val_samples):\n",
    "    train_dfs = []\n",
    "    val_dfs = []\n",
    "    test_dfs = []\n",
    "    for category, group in data_frame.groupby(\"category\"):\n",
    "        train_df = group.sample(n=train_samples)\n",
    "        group = group.drop(train_df.index)\n",
    "        val_df = group.sample(n=val_samples)\n",
    "        group = group.drop(val_df.index)\n",
    "        test_df = group\n",
    "        train_dfs.append(train_df)\n",
    "        val_dfs.append(val_df)\n",
    "        test_dfs.append(test_df)\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    val_df = pd.concat(val_dfs)\n",
    "    test_df = pd.concat(test_dfs)\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码，产生掩蔽，对label编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, data_frame, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.data_frame[\"category\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.root_dir, self.data_frame.iloc[idx][\"filename\"])\n",
    "        label = self.data_frame.iloc[idx][\"category\"]\n",
    "\n",
    "        # Load audio data and perform any desired transformations\n",
    "        sig, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        sig_t = torch.tensor(sig)\n",
    "        padding_mask = torch.zeros(1, sig_t.shape[0]).bool().squeeze(0)\n",
    "        if self.transform:\n",
    "            sig_t = self.transform(sig_t)\n",
    "\n",
    "        # Encode label as integer\n",
    "        label = self.label_encoder.transform([label])[0]\n",
    "\n",
    "        return sig_t, padding_mask, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继承LightningDataModule，产生trainset和valset的dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECS50DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str = \"/data/ESC-50-master/audio/\",\n",
    "        csv_file: str = \"/data/ESC-50-master/meta/esc50.csv\",\n",
    "        batch_size: int = 8,\n",
    "        split_ratio=0.8,\n",
    "        transform=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.split_ratio = split_ratio\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_frame = pd.read_csv(self.csv_file)\n",
    "        data_frame = data_frame.sample(frac=1).reset_names(\n",
    "            drop=True\n",
    "        )  # shuffle the data frame\n",
    "        split_names = int(len(data_frame) * self.split_ratio)\n",
    "        self.train_set = data_frame.iloc[:split_names, :]\n",
    "        self.val_set = data_frame.iloc[split_names:, :]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_df = AudioDataset(\n",
    "            root_dir=self.root_dir, data_frame=self.train_set, transform=self.transform\n",
    "        )\n",
    "\n",
    "        return DataLoader(train_df, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_df = AudioDataset(\n",
    "            root_dir=self.root_dir, data_frame=self.val_set, transform=self.transform\n",
    "        )\n",
    "\n",
    "        return DataLoader(val_df, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "def logger_init(log_level=logging.INFO,\n",
    "                log_dir='./ResultFile/',\n",
    "                ):\n",
    "    # 指定路径\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "  \n",
    "    log_path = os.path.join(log_dir, '_' + str(datetime.now())[:10] + '.txt')\n",
    "    formatter = '[%(asctime)s] - %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(level=log_level,\n",
    "                        format=formatter,\n",
    "                        datefmt='%Y-%d-%m %H:%M:%S',\n",
    "                        handlers=[logging.FileHandler(log_path),\n",
    "                                logging.StreamHandler(sys.stdout)]\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_row(file_name,row_num):\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        row=list(reader)\n",
    "    return row[row_num]\n",
    "\n",
    "def csv_reader_cl(file_name,clo_num):\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        column = [row[clo_num] for row in reader]\n",
    "    return column\n",
    "\n",
    "import csv\n",
    "csv_path=\"E:\\\\Shilong\\\\murmur\\\\dataset_all\\\\training_data.csv\"\n",
    "# csv_path=\"E:\\\\Shilong\\\\murmur\\\\circor_digiscope_dataset\\\\training_data.csv\"\n",
    "\n",
    "# get dataset tag from table\n",
    "row_line=csv_reader_row(csv_path,0)\n",
    "tag_list=list()\n",
    "# get names for 'Patient ID' and 'Outcome'\n",
    "tag_list.append(row_line.names('Patient ID'))\n",
    "tag_list.append(row_line.names('Murmur'))\n",
    "tag_list.append(row_line.names('Murmur locations'))\n",
    "tag_list.append(row_line.index('Systolic murmur timing'))\n",
    "tag_list.append(row_line.index('Diastolic murmur timing'))\n",
    "# for tag_names in tag_list:\n",
    "id_data=csv_reader_cl(csv_path,tag_list[0])\n",
    "Murmur=csv_reader_cl(csv_path,tag_list[1])\n",
    "Murmur_locations=csv_reader_cl(csv_path,tag_list[2])\n",
    "Systolic_murmur_timing=csv_reader_cl(csv_path,tag_list[3])\n",
    "Diastolic_murmur_timing=csv_reader_cl(csv_path,tag_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_id  = [out for out,Murmur in enumerate(Murmur) if Murmur=='Absent']\n",
    "present_id = [out for out,Murmur in enumerate(Murmur) if Murmur=='Present']\n",
    "# print(dict(enumerate(id_data[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def index_load(tsvname):\n",
    "    \"\"\"读取tsv文件内容,不需要close函数\"\"\"\n",
    "    with open(tsvname, 'r') as f:\n",
    "        txt_data = f.read()\n",
    "    head=['start','end','period']\n",
    "    data=txt_data.split('\\n')[:-1]\n",
    "    #遍历每一行\n",
    "    for l in data:\n",
    "        sgmt=l.split('\\t')\n",
    "        if sgmt[2]!='0':\n",
    "            head=np.vstack([head,sgmt])\n",
    "    return head[1:]\n",
    "\n",
    "def state_div(tsvname,wavname,state_path,index):\n",
    "    index_file=index_load(tsvname)\n",
    "    recording, fs = librosa.load(wavname,sr=4000)\n",
    "    num=0\n",
    "    start_index2=0\n",
    "    end_index2=0\n",
    "    start_index4=0\n",
    "    end_index4=0\n",
    "\n",
    "    for i in range(index_file.shape[0]-3):\n",
    "        if index_file[i][2]=='2'and index_file[i+2][2]=='4':\n",
    "            start_index2=float(index_file[i][0])*fs\n",
    "            end_index2=float(index_file[i][1])*fs\n",
    "            start_index4=float(index_file[i+2][0])*fs\n",
    "            end_index4=float(index_file[i+2][1])*fs\n",
    "            num=num+1\n",
    "            #  解决出现_0.wav的问题\n",
    "            print(start_index2,end_index2,start_index4,end_index4)            \n",
    "            print(\"=============================================\")\n",
    "            print(\"wav name: \"+wavname)        \n",
    "            buff2 = recording[int(start_index2) :int(end_index2) ]  # 字符串索引切割\n",
    "            buff4 = recording[int(start_index4) :int(end_index4) ]  # 字符串索引切割\n",
    "            print(\"buff2 len: \"+str(len(buff2)),\"buff4 len: \"+str(len(buff4)))\n",
    "            # soundfile.write(state_path+\"{}_{}_{}.wav\".format(index,'Systolic' ,num),buff2,fs)\n",
    "            # soundfile.write(state_path+\"{}_{}_{}.wav\".format(index,'Diastolic',num),buff4,fs)\n",
    "\n",
    "def period_div(path,murmur,patient_id_list,positoin):\n",
    "    for mur in murmur:\n",
    "        for patient_id in patient_id_list:\n",
    "            for pos in positoin:\n",
    "                dir_path=path+mur+patient_id+\"\\\\\"+patient_id+pos\n",
    "                tsv_path=dir_path+\".tsv\"\n",
    "                wav_path=dir_path+\".wav\"\n",
    "                if os.path.exists(tsv_path):\n",
    "                    state_div(tsv_path,wav_path,dir_path+\"\\\\\",patient_id+pos)\n",
    "\n",
    "def get_patientid(csv_path):\n",
    "    # 'import csv' is required\n",
    "    with open(csv_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        id = [row[0] for row in reader]   # weight 同列的数据\n",
    "    return id\n",
    "\n",
    "murmur=[\"Absent\\\\\",\"Present\\\\\"]\n",
    "positoin=['_AV','_MV','_PV','_TV']\n",
    "folder_path=r'E:\\Shilong\\murmur\\03_circor_states\\\\'\n",
    "absent_csv_path=r'E:\\Shilong\\murmur\\03_Classifier\\LM\\BEATs\\absent_id.csv'\n",
    "absent_patient_id=get_patientid(absent_csv_path)\n",
    "period_div(folder_path,murmur,absent_patient_id,positoin)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.compliance.kaldi as ta_kaldi\n",
    "import torch\n",
    "\n",
    "waveform = torch.randn(1, 2800)\n",
    "fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_len(dir_path,csv_path,Murmur:str,id_data,Murmur_locations):\n",
    "    slen=[]\n",
    "    dlen=[]\n",
    "    # label=[]\n",
    "    if not os.path.exists(csv_path):\n",
    "        os.makedirs(csv_path)\n",
    "\n",
    "    for root,dir,file in os.walk(dir_path):\n",
    "        for subfile in file:\n",
    "            wav_path=os.path.join(root,subfile)            \n",
    "            if os.path.exists(wav_path):\n",
    "                # 数据读取\n",
    "                print(\"reading: \"+subfile)\n",
    "                y, sr = librosa.load(wav_path, sr=4000)\n",
    "                y_16k = librosa.resample(y=y, orig_sr=sr, target_sr=16000)\n",
    "                print(\"y_16k size: \"+str(y_16k.size))\n",
    "                if subfile.split('_')[2] == 'Systolic':\n",
    "                    slen.append(y_16k.size)\n",
    "                else:\n",
    "                    dlen.append(y_16k.size)    \n",
    "    return np.array(slen),np.array(dlen)\n",
    "\n",
    "slen,dlen=cal_len(absent_train_path,absent_train_csv_path,'Absent',id_data,Murmur_locations)# absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a=np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "b=torch.tensor(a)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_train_csv_path = r'D:\\Shilong\\murmur\\03_circor_states\\train_csv'\n",
    "absent_test_csv_path = r'D:\\Shilong\\murmur\\03_circor_states\\test_csv'\n",
    "present_train_csv_path = r'D:\\Shilong\\murmur\\03_circor_states\\train_csv'\n",
    "present_test_csv_path = r'D:\\Shilong\\murmur\\03_circor_states\\test_csv'\n",
    "\n",
    "filepath=r'D:\\Shilong\\murmur\\03_circor_states'\n",
    "absent_train_path=r'D:\\Shilong\\murmur\\03_circor_states\\train\\Absent'\n",
    "absent_test_path=r'D:\\Shilong\\murmur\\03_circor_states\\test\\Absent'\n",
    "present_train_path=r'D:\\Shilong\\murmur\\03_circor_states\\train\\Present'\n",
    "present_test_path=r'D:\\Shilong\\murmur\\03_circor_states\\test\\Present'\n",
    "\n",
    "atslen,atdlen=cal_len(absent_train_path,absent_train_csv_path,'Absent',id_data,Murmur_locations)# absent\n",
    "avslen,avdlen=cal_len(absent_test_path,absent_test_csv_path,'Absent',id_data,Murmur_locations)# absent\n",
    "ptslen,ptdlen=cal_len(present_train_path,present_train_csv_path,'Present',id_data,Murmur_locations)# absent\n",
    "pvslen,pvdlen=cal_len(present_test_path,present_test_csv_path,'Present',id_data,Murmur_locations)# absent\n",
    "\n",
    "atslen_path=r'D:\\Shilong\\murmur\\03_circor_states\\atslen.csv'\n",
    "atdlen_path=r'D:\\Shilong\\murmur\\03_circor_states\\atdlen.csv'\n",
    "avslen_path=r'D:\\Shilong\\murmur\\03_circor_states\\avslen.csv'\n",
    "avdlen_path=r'D:\\Shilong\\murmur\\03_circor_states\\avdlen.csv'\n",
    "ptslen_path=r'D:\\Shilong\\murmur\\03_circor_states\\ptslen.csv'\n",
    "ptdlen_path=r'D:\\Shilong\\murmur\\03_circor_states\\ptdlen.csv'\n",
    "pvslen_path=r'D:\\Shilong\\murmur\\03_circor_states\\pvslen.csv'\n",
    "pvdlen_path=r'D:\\Shilong\\murmur\\03_circor_states\\pvdlen.csv'\n",
    "\n",
    "pd.DataFrame(atslen).to_csv(atslen_path, index=False, header=False)\n",
    "pd.DataFrame(atdlen).to_csv(atdlen_path, index=False, header=False)\n",
    "pd.DataFrame(avslen).to_csv(avslen_path, index=False, header=False)\n",
    "pd.DataFrame(avdlen).to_csv(avdlen_path, index=False, header=False)\n",
    "pd.DataFrame(ptslen).to_csv(ptslen_path, index=False, header=False)\n",
    "pd.DataFrame(ptdlen).to_csv(ptdlen_path, index=False, header=False)\n",
    "pd.DataFrame(pvslen).to_csv(pvslen_path, index=False, header=False)\n",
    "pd.DataFrame(pvdlen).to_csv(pvdlen_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "absent_1=0\n",
    "persent_1=0\n",
    "absent_2=0\n",
    "persent_2=0\n",
    "\n",
    "\n",
    "file_path_train=r'D:\\Shilong\\murmur\\03_circor_states\\train'\n",
    "file_path_test=r'D:\\Shilong\\murmur\\03_circor_states\\test'\n",
    "target_dir_train_a=r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\absent'\n",
    "target_dir_train_p=r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\present'\n",
    "target_dir_test_a=r'D:\\Shilong\\murmur\\03_circor_states\\testset\\absent'\n",
    "target_dir_test_p=r'D:\\Shilong\\murmur\\03_circor_states\\testset\\present'\n",
    "\n",
    "for root,dir,file in os.walk(file_path_train):\n",
    "    for subfile in file:\n",
    "        files=os.path.join(root,subfile)\n",
    "        print(subfile)\n",
    "        state=subfile.split(\"_\")[4]        \n",
    "        if state=='Absent':\n",
    "            shutil.copy(files, target_dir_train_a + \"\\\\\")\n",
    "        if state=='Present':\n",
    "            shutil.copy(files, target_dir_train_p + \"\\\\\")\n",
    "\n",
    "for root,dir,file in os.walk(file_path_test):\n",
    "    for subfile in file:\n",
    "        files=os.path.join(root,subfile)\n",
    "        print(subfile)\n",
    "        state=subfile.split(\"_\")[4]        \n",
    "        if state=='Absent':\n",
    "            shutil.copy(files, target_dir_test_a + \"\\\\\")\n",
    "        if state=='Present':\n",
    "            shutil.copy(files, target_dir_test_p + \"\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_rate = np.random.uniform(0.7,1.3)\n",
    "wav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\n",
    "print('speed rate: %.3f' % speed_rate, '(lower is faster)')\n",
    "if len(wav_speed_tune) < 16000:\n",
    "    pad_len = 16000 - len(wav_speed_tune)\n",
    "    wav_speed_tune = np.r_[np.random.uniform(-0.001,0.001,int(pad_len/2)),\n",
    "                           wav_speed_tune,\n",
    "                           np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2)))]\n",
    "else: \n",
    "    cut_len = len(wav_speed_tune) - 16000\n",
    "    wav_speed_tune = wav_speed_tune[int(cut_len/2):int(cut_len/2)+16000]\n",
    "print('wav length: ', wav_speed_tune.shape[0])\n",
    "ipd.Audio(wav_speed_tune, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix background noise & volume tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_files = os.listdir('../input/train/audio/_background_noise_/')\n",
    "bg_files.remove('README.md')\n",
    "chosen_bg_file = bg_files[np.random.randint(6)]\n",
    "bg, sr = librosa.load('../input/train/audio/_background_noise_/'+chosen_bg_file, sr=None)\n",
    "print(chosen_bg_file,'|', bg.shape[0], bg.max(), bg.min())\n",
    "ipd.Audio(bg, rate=sr) # !! be prepared when playing the noise, bacause it's so ANNOYING !!\n",
    "\n",
    "start_ = np.random.randint(bg.shape[0]-16000)\n",
    "bg_slice = bg[start_ : start_+16000]\n",
    "wav_with_bg = wav * np.random.uniform(0.8, 1.2) + \\\n",
    "              bg_slice * np.random.uniform(0, 0.1)\n",
    "ipd.Audio(wav_with_bg, rate=sr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stretching the sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile\n",
    "def stretch(data, rates):   \n",
    "    data = librosa.effects.time_stretch(data, rate=rates)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def get_spectrogram(wav):\n",
    "    D = librosa.stft(wav, n_fft=480, hop_length=160,\n",
    "                     win_length=480, window='hamming')\n",
    "    spect, phase = librosa.magphase(D)\n",
    "    return spect\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    data,sr=librosa.load(r'D:\\Shilong\\murmur\\03_circor_states\\Absent\\2530\\2530_AV\\.wav', sr=4000)\n",
    "    data_stretch =stretch(data, 2)\n",
    "\n",
    "\n",
    "    data_stretch_log_spect = 20*np.log10(get_spectrogram(data_stretch))\n",
    "    data_log_spect = np.log(get_spectrogram(data))\n",
    "    # soundfile.write(r\"D:\\Shilong\\murmur\\03_circor_states\\Absent\\2530\\2530AV.wav\",\n",
    "    #             data,\n",
    "    #             sr*2,\n",
    "    #         )\n",
    "    soundfile.write(r\"D:\\Shilong\\murmur\\03_circor_states\\Absent\\2530_AV.wav\",\n",
    "    data_stretch,\n",
    "    sr,\n",
    "            )\n",
    "\n",
    "   \n",
    "    plt.title('spectrogram of origin audio and stretch audio')\n",
    "    plt.subplot(2, 1, 1)   \n",
    "    plt.imshow(data_log_spect, aspect='auto', origin='lower',)    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(data_stretch_log_spect, aspect='auto', origin='lower',)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUDIO FEATURE AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "wav_path = r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\volume2\\9979_AV_Systolic_1_Present_Holosystolic.wav_volume2.wav'\n",
    "wav_path2 = r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\present\\9979_AV_Systolic_1_Present_Holosystolic.wav'\n",
    "def get_spectrogram(\n",
    "    n_fft=400,\n",
    "    win_len=None,\n",
    "    hop_len=None,\n",
    "    power=2.0,\n",
    "    path=wav_path,\n",
    "):\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    waveform = torchaudio.transforms.Resample(4000, 16000)(waveform)\n",
    "    spectrogram = T.Spectrogram(\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_len,\n",
    "        hop_length=hop_len,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=power,\n",
    "    )\n",
    "    melspec=T.MelSpectrogram(\n",
    "        sample_rate = 16000,\n",
    "        n_fft= 200,       \n",
    "        )\n",
    "    return melspec(waveform)\n",
    "\n",
    "\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "    \n",
    "# TimeStretch\n",
    "\n",
    "spec = get_spectrogram()\n",
    "spec2 = get_spectrogram(path=wav_path2)\n",
    "stretch = T.TimeStretch(n_freq = 128)\n",
    "\n",
    "\n",
    "rate = 1.2\n",
    "spec_ = stretch(spec, rate)\n",
    "\n",
    "plot_spectrogram(torch.abs(spec_[0]), title=f\"Stretched x{rate}\", aspect=\"equal\", xmax=18)\n",
    "\n",
    "plot_spectrogram(torch.abs(spec[0]), title=\"origin\", aspect=\"equal\", xmax=18)\n",
    "plot_spectrogram(torch.abs(spec2[0]), title=\"origin2\", aspect=\"equal\", xmax=18)\n",
    "rate = 0.8\n",
    "spec_ = stretch(spec, rate)\n",
    "plot_spectrogram(torch.abs(spec_[0]), title=f\"Stretched x{rate}\", aspect=\"equal\", xmax=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "SAMPLE_WAV_SPEECH_PATH = r\"D:\\Shilong\\murmur\\01_dataset\\04_newDataset\\Absent\\2530\\2530_AV.wav\"\n",
    "def get_spectrogram(\n",
    "    n_fft=400,\n",
    "    win_len=None,\n",
    "    hop_len=None,\n",
    "    power=2.0,\n",
    "):\n",
    "    waveform, _ = torchaudio.load(SAMPLE_WAV_SPEECH_PATH)\n",
    "    # waveform = torchaudio.transforms.Resample(4000, 16000)(waveform)\n",
    "    spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=4000\n",
    "    )    \n",
    "    return waveform\n",
    "\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    # axs.set_title(title or \"Spectrogram (db)\")\n",
    "    # axs.set_ylabel(ylabel)\n",
    "    # axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "    axs.axes.xaxis.set_visible(False)\n",
    "    axs.axes.yaxis.set_visible(False)\n",
    "    axs.spines['top'].set_visible(False)\n",
    "    axs.spines['right'].set_visible(False)\n",
    "    axs.spines['bottom'].set_visible(False)\n",
    "    axs.spines['left'].set_visible(False)\n",
    "\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    # fig.colorbar(im, ax=axs)\n",
    "    # plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "    # plt.margins(0, 0)\n",
    "    plt.show(block=False)\n",
    "\n",
    "spec = get_spectrogram(power=None)\n",
    "stretch = T.TimeStretch(n_freq=128)\n",
    "xmax=400\n",
    "plot_spectrogram(spec, title=\"mel\", aspect=\"equal\", xmax=xmax)\n",
    "\n",
    "# rate = 1.2\n",
    "# spec_ = stretch(spec, rate)\n",
    "# plot_spectrogram(torch.abs(spec_[0]), title=f\"Stretched x{rate}\", aspect=\"equal\", xmax=xmax)\n",
    "\n",
    "# rate = 0.9\n",
    "# spec_ = stretch(spec, rate)\n",
    "# plot_spectrogram(torch.abs(spec_[0]), title=f\"Stretched x{rate}\", aspect=\"equal\", xmax=xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, sr = librosa.load(r\"D:\\Shilong\\murmur\\01_dataset\\04_newDataset\\Absent\\2530\\2530_AV.wav\", sr=4000)\n",
    "# samples = samples[6000:16000]\n",
    "\n",
    "# print(len(samples), sr)\n",
    "time = np.arange(0, len(samples)) * (1.0 / sr)\n",
    "\n",
    "axs=plt.plot(time, samples)\n",
    "axs.axes.xaxis.set_visible(False)\n",
    "axs.axes.yaxis.set_visible(False)\n",
    "axs.spines['top'].set_visible(False)\n",
    "axs.spines['right'].set_visible(False)\n",
    "axs.spines['bottom'].set_visible(False)\n",
    "axs.spines['left'].set_visible(False)\n",
    "\n",
    "# plt.savefig(\"your dir\\语音信号时域波形图\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wav mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "# \\2530_AV_Diastolic_10_Absent_nan\n",
    "waveform, _ = torchaudio.load(r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\present\\9979_AV_Systolic_1_Present_Holosystolic.wav')\n",
    "wavform=torchaudio.transforms.Resample(orig_freq=4000, new_freq=16000)(waveform)\n",
    "fbank = torchaudio.compliance.kaldi.fbank(\n",
    "                waveform,\n",
    "                num_mel_bins=128,\n",
    "                sample_frequency=16000,\n",
    "                frame_length=25,\n",
    "                frame_shift=10,\n",
    "            )\n",
    "freqm=1# 横向\n",
    "timem=1# 纵向\n",
    "freqm = torchaudio.transforms.FrequencyMasking(freqm)\n",
    "timem = torchaudio.transforms.TimeMasking(timem)\n",
    "fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "# NOTE this line, this is the trick, new torchaudio expect [1, freq, time] while old support [freq, time]\n",
    "# comment this line will lead to an issue.\n",
    "fbank = fbank.squeeze(0)\n",
    "print(fbank.shape)\n",
    "if freqm != 0:\n",
    "    fbank = freqm(fbank)\n",
    "if timem != 0:\n",
    "    fbank = timem(fbank)\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "\n",
    "plot_spectrogram(fbank, title=f\"Masking\", aspect=\"equal\", xmax=18)\n",
    "# plt.imshow(fbank[0].cpu().numpy())\n",
    "fbank = fbank.squeeze(0)\n",
    "print(fbank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import TimeStretch\n",
    "\n",
    "# 加载音频文件\n",
    "waveform, sample_rate = torchaudio.load(r'D:\\Shilong\\murmur\\03_circor_states\\Absent\\2530\\2530_AV.wav')\n",
    "# 转换为一维张量\n",
    "waveform = waveform[0, :]\n",
    "\n",
    "# 创建时间拉伸的变换器\n",
    "time_stretch = TimeStretch()\n",
    "\n",
    "# 设置拉伸因子\n",
    "stretch_factor = 1.5\n",
    "\n",
    "# 创建与波形数据样本数相匹配的拉伸因子张量\n",
    "num_samples = waveform.size(0)\n",
    "stretch_factor_tensor = torch.tensor([stretch_factor]).expand(num_samples)\n",
    "\n",
    "# 将拉伸因子转换为布尔值\n",
    "stretch_factor_tensor = stretch_factor_tensor.bool()\n",
    "\n",
    "# 进行时间拉伸\n",
    "stretched_waveform = time_stretch(waveform, stretch_factor_tensor)\n",
    "\n",
    "# 保存拉伸后的音频文件\n",
    "torchaudio.save('stretched_audio.wav', stretched_waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "speed_factor=1.2\n",
    "time_path=r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\volume2'\n",
    "path=r'D:\\Shilong\\murmur\\03_circor_states\\trainset\\present'\n",
    "for root,dir,file in os.walk(path):\n",
    "    for filename in file:\n",
    "        print(\"processing \"+filename)\n",
    "        wav_path=os.path.join(root,filename)\n",
    "        data, sr = librosa.load(wav_path, sr=None)\n",
    "        # data_time_stretch=librosa.effects.time_stretch(data, rate=speed_factor)\n",
    "        sf.write(os.path.join(time_path,filename+'_volume2.wav'),data,sr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "# 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练的BERT模型和tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载微调数据集\n",
    "train_dataset = YOUR_TRAIN_DATASET  # 替换为你自己的训练数据集\n",
    "val_dataset = YOUR_VALIDATION_DATASET  # 替换为你自己的验证数据集\n",
    "\n",
    "# 定义批处理大小和训练时的批处理器\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义优化器和训练参数\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "# 将模型移动到设备上\n",
    "model.to(device)\n",
    "\n",
    "# 训练和微调过程\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        # 准备数据\n",
    "        inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        labels = batch['label'].to(device)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # 在验证集上进行评估\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            labels = batch['label'].to(device)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels).item()\n",
    "            total_predictions += len(labels)\n",
    "\n",
    "    # 打印训练和验证信息\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_accuracy:.2%}\")\n",
    "\n",
    "# 保存微调后的模型\n",
    "model.save_pretrained(\"fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同层对学习结果的贡献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 梯度类别激活图（Grad-CAM）：\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 加载预训练的模型\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 读取并预处理图像\n",
    "image = Image.open('image.jpg')\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# 将输入图像转换为梯度可计算的形式\n",
    "input_batch.requires_grad_()\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_batch)\n",
    "target_class = torch.argmax(output)\n",
    "\n",
    "# 反向传播计算梯度\n",
    "model.zero_grad()\n",
    "output[0, target_class].backward()\n",
    "\n",
    "# 获取目标层的梯度\n",
    "grads = model.get_activations_gradient()\n",
    "\n",
    "# 获取目标层的特征图\n",
    "target_activations = model.get_activations(input_batch).detach()\n",
    "\n",
    "# 计算类别激活图\n",
    "weights = torch.mean(grads, dim=(2, 3))[0]\n",
    "grad_cam = torch.sum(weights * target_activations, dim=1).relu()\n",
    "\n",
    "# 可视化类别激活图\n",
    "grad_cam = torch.nn.functional.interpolate(grad_cam.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False)\n",
    "grad_cam = torch.nn.functional.normalize(grad_cam, dim=1)\n",
    "\n",
    "# 可以根据需要使用matplotlib等库进行可视化展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from model_M7 import mobilenet_M7_large\n",
    "import json\n",
    "\n",
    "#导入预先训练好的网络\n",
    "model = mobilenet_M7_large(num_classes=4)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_weight_path = \"M7/best_M7.pth\"  #修改自己的权重文件\n",
    "model.load_state_dict(torch.load(model_weight_path))\n",
    "# model = torch.load(model_weight_path, map_location=device)\n",
    "model.eval()\n",
    "print(model) #可以注释掉\n",
    "\n",
    "#读取一张图片，对其可视化\n",
    "im = Image.open(\"test_image/0.jpg\")\n",
    "imarray = np.asarray(im) / 255.0\n",
    "# plt.figure()\n",
    "# plt.imshow(imarray)\n",
    "# plt.show()\n",
    "\n",
    "#数据预处理\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "input_im = data_transforms(im).unsqueeze(0)\n",
    "print(\"input_im.shape:\",input_im.shape)\n",
    "\n",
    "#定义辅助函数，获取指定层名称的特征，定义钩子hook\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model,input,output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "#获取中间卷积后的图像特征\n",
    "model.eval()\n",
    "#获取网络中第6层，经过第一次注意力后的特征映射，第一张特征图。\n",
    "model.features[6].register_forward_hook(get_activation(\"SqueezeExcitation\"))\n",
    "_ = model(input_im)\n",
    "SqueezeExcitation = activation[\"SqueezeExcitation\"]\n",
    "print(\"获取特征的尺寸为：\",SqueezeExcitation.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for ii in range(SqueezeExcitation.shape[1]):\n",
    "    # 可视化每张手写体\n",
    "    plt.subplot(4, 10, ii + 1)\n",
    "    plt.imshow(SqueezeExcitation.data.numpy()[0, ii, :, :], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.savefig(\"SqueezeExcitation6 map\")\n",
    "#获取更深层次的卷积后的图像特征\n",
    "model.eval()\n",
    "#获取网络中第14层，经过第二次注意力的特征映射，第二张特征图。\n",
    "model.features[14].register_forward_hook(get_activation(\"SqueezeExcitation\"))\n",
    "_ = model(input_im)\n",
    "SqueezeExcitation = activation[\"SqueezeExcitation\"]\n",
    "print(\"获取特征的尺寸为：\",SqueezeExcitation.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))   \n",
    "for ii in range(40):\n",
    "    # 可视化每张手写体\n",
    "    plt.subplot(4, 10, ii + 1)\n",
    "    plt.imshow(SqueezeExcitation.data.numpy()[0, ii, :, :], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.savefig(\"SqueezeExcitation14 map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function \n",
    "def train(num_epochs): \n",
    "    best_accuracy = 0.0 \n",
    "     \n",
    "    print(\"Begin training...\") \n",
    "    for epoch in range(1, num_epochs+1): \n",
    "        running_train_loss = 0.0 \n",
    "        running_accuracy = 0.0 \n",
    "        running_vall_loss = 0.0 \n",
    "        total = 0 \n",
    " \n",
    "        # Training Loop \n",
    "        for data in train_loader: \n",
    "        #for data in enumerate(train_loader, 0): \n",
    "            inputs, outputs = data  # get the input and real species as outputs; data is a list of [inputs, outputs] \n",
    "            optimizer.zero_grad()   # zero the parameter gradients          \n",
    "            predicted_outputs = model(inputs)   # predict output from the model \n",
    "            train_loss = loss_fn(predicted_outputs, outputs)   # calculate loss for the predicted output  \n",
    "            train_loss.backward()   # backpropagate the loss \n",
    "            optimizer.step()        # adjust parameters based on the calculated gradients \n",
    "            running_train_loss +=train_loss.item()  # track the loss value \n",
    " \n",
    "        # Calculate training loss value \n",
    "        train_loss_value = running_train_loss/len(train_loader) \n",
    " \n",
    "        # Validation Loop \n",
    "        with torch.no_grad(): \n",
    "            model.eval() \n",
    "            for data in validate_loader: \n",
    "                inputs, outputs = data \n",
    "                predicted_outputs = model(inputs) \n",
    "                val_loss = loss_fn(predicted_outputs, outputs) \n",
    "                \n",
    "                # The label with the highest value will be our prediction \n",
    "                _, predicted = torch.max(predicted_outputs, 1) \n",
    "                running_vall_loss += val_loss.item()  \n",
    "                total += outputs.size(0) \n",
    "                running_accuracy += (predicted == outputs).sum().item() \n",
    "\n",
    "        # Calculate validation loss value \n",
    "        val_loss_value = running_vall_loss/len(validate_loader) \n",
    "                \n",
    "        # Calculate accuracy as the number of correct predictions in the validation batch divided by the total number of predictions done.  \n",
    "        accuracy = (100 * running_accuracy / total)     \n",
    "\n",
    "        # Save the model if the accuracy is the best \n",
    "        if accuracy > best_accuracy: \n",
    "            saveModel() \n",
    "            best_accuracy = accuracy \n",
    "\n",
    "        # Print the statistics of the epoch \n",
    "        print('Completed training batch', epoch, 'Training Loss is: %.4f' %train_loss_value, 'Validation Loss is: %.4f' %val_loss_value, 'Accuracy is %d %%' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a=torch.tensor([10,10000])\n",
    "print(F.softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"my dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self, wavlabel, wavdata):\n",
    "        # 直接传递data和label\n",
    "        # self.len = wavlen\n",
    "        self.data = torch.from_numpy(wavdata)\n",
    "        self.label = torch.from_numpy(wavlabel)\n",
    "        self.id = torch.from_numpy(wavidx)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        dataitem = torch.Tensor(self.data[index])\n",
    "        labelitem = torch.Tensor(self.label[index])\n",
    "        return dataitem.float(), labelitem.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_idx(self, index):\n",
    "        iditem = torch.Tensor(self.id[index])\n",
    "        return iditem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多尺度熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sampEn(L:np.array, std : float ,m: int= 2, r: float = 0.15):\n",
    "    \"\"\" \n",
    "    计算时间序列的样本熵\n",
    "    \n",
    "    Input: \n",
    "        L: 时间序列\n",
    "        std: 原始序列的标准差\n",
    "        m: 1或2\n",
    "        r: 阈值\n",
    "        \n",
    "    Output: \n",
    "        SampEn\n",
    "    \"\"\"\n",
    "    N = len(L)\n",
    "    B = 0.0\n",
    "    A = 0.0\n",
    "\n",
    "    # Split time series and save all templates of length m\n",
    "    xmi = np.array([L[i:i+m] for i in range(N-m)])\n",
    "    xmj = np.array([L[i:i+m] for i in range(N-m+1)])\n",
    "    \n",
    "    # Save all matches minus the self-match, compute B\n",
    "    B = np.sum([np.sum(np.abs(xmii-xmj).max(axis=1) <= r * std)-1 for xmii in xmi])\n",
    "    # Similar for computing A\n",
    "    m += 1\n",
    "    xm = np.array([L[i:i+m] for i in range(N-m+1)])\n",
    "    \n",
    "    A = np.sum([np.sum(np.abs(xmi-xm).max(axis=1) <= r * std)-1 for xmi in xm])\n",
    "    # Return SampEn\n",
    "    return -np.log(A/B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import librosa\n",
    "# from SampEn import sampEn\n",
    "\n",
    "\n",
    "def MSE(signal , max_scale:int = 20):\n",
    "    result = []\n",
    "    length = len(signal)\n",
    "    std = np.std(signal)\n",
    "\n",
    "    for scale in range(1 , max_scale + 1):\n",
    "        # 确定截取的长度\n",
    "        length = int(len(signal) / scale) - 1\n",
    "        # 分段取平均\n",
    "        scale_i = signal[ : len(signal) : scale][:length]\n",
    "        for i in range(1,scale):\n",
    "            scale_i = scale_i + signal[i: len(signal) : scale][:length]\n",
    "        scale_i = scale_i / scale\n",
    "        #计算样本熵\n",
    "        result.append(sampEn(scale_i, std ,r = 0.15))\n",
    "        print(\"scale:\" , scale, 'SampEn' , result[-1])\n",
    "    return result\n",
    "\n",
    "\n",
    "# white_noise = pd.read_csv(\"white_noise.csv\").loc[:,'0']\n",
    "# white_noise = white_noise.to_numpy()\n",
    "\n",
    "data,_=librosa.load(r'C:\\Users\\lsl198\\Desktop\\倒放.wav', sr=4000)\n",
    "begin = time.time()\n",
    "entropy = MSE(data , 20)\n",
    "end = time.time()\n",
    "\n",
    "print(\"用时：\" , int((end - begin)/60) ,'min', (end - begin) % 60 , 's')\n",
    "print(entropy)\n",
    "\n",
    "plt.plot(entropy)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "a=[1,2,3]\n",
    "b=['a','b','c']\n",
    "pd.DataFrame(zip(a,b)).to_csv(\n",
    "    \"absent_train_dic.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每一段进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def segment_classifier(result_list_1=[]):\n",
    "    npy_path_padded = r\"D:\\Shilong\\murmur\\01_dataset\\01_s1s2\\npyFile_padded\\normalized\\list_npy_files\"\n",
    "    absent_test_index = np.load(\n",
    "        npy_path_padded + r\"\\absent_test_index_norm.npy\", allow_pickle=True\n",
    "    )\n",
    "    present_test_index = np.load(\n",
    "        npy_path_padded + r\"\\present_test_index_norm.npy\", allow_pickle=True\n",
    "    )\n",
    "    absent_test_names = np.load(\n",
    "        npy_path_padded + r\"\\absent_test_names_norm.npy\", allow_pickle=True\n",
    "    )\n",
    "    present_test_names = np.load(\n",
    "        npy_path_padded + r\"\\present_test_names_norm.npy\", allow_pickle=True\n",
    "    )\n",
    "    absent_test_dic = dict(zip(absent_test_names,absent_test_index, ))\n",
    "    present_test_dic = dict(zip(present_test_names,present_test_index, ))\n",
    "    # 所有测试数据的字典\n",
    "    test_dic={**absent_test_dic,**present_test_dic}\n",
    "    #创建id_pos:idx的字典 \n",
    "    id_idx_dic={} \n",
    "    # 遍历test_dic，生成id_pos:idx的字典\n",
    "    for file_name,data_index in test_dic.items():\n",
    "        id_pos=file_name.split('_')[0]+'_'+file_name.split('_')[1]\n",
    "        if not id_pos in id_idx_dic.keys():#如果id_pos不在字典中，就创建一个新的键值对\n",
    "            id_idx_dic[id_pos]=[data_index]\n",
    "        else: # 如果id_pos在字典中，就把value添加到对应的键值对的值中\n",
    "            id_idx_dic[id_pos].append(data_index)\n",
    "\n",
    "    # 这里result_list_1列表，用来存储分类结果为1对应的id\n",
    "    # 创建一个空字典，用来存储分类结果\n",
    "    result_dic={}\n",
    "    # 这样就生成了每个听诊区对应的数据索引，然后就可以根据索引读取数据了\n",
    "    for id_pos,data_index in id_idx_dic.items():\n",
    "        # 创建空列表用于保存数据索引对应的值\n",
    "        value_list=[]\n",
    "        # 遍历这个id_pos对应的所有数据索引\n",
    "        for idx in data_index:\n",
    "            # 根据索引读取数据\n",
    "            if idx in result_list_1:\n",
    "                value_list.append(1)\n",
    "            else:\n",
    "                value_list.append(0)\n",
    "        # 计算平均值作为每一段的最终分类结果，大于0.5就是1，小于0.5就是0\n",
    "        result_dic[id_pos]=value_list.mean()\n",
    "    return result_dic\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照患者分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "id_idx_dic={}\n",
    "id_pos=11\n",
    "id_idx_dic[id_pos]=['aa']\n",
    "id_idx_dic[id_pos].append('bb')\n",
    "\n",
    "print(id_idx_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def csv_reader_cl(file_name, clo_num):\n",
    "    with open(file_name, encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column = [row[clo_num] for row in reader]\n",
    "    return column\n",
    "def csv_reader_row(file_name, row_num):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        row = list(reader)\n",
    "    return row[row_num]\n",
    "absent_test_id_path = r\"D:\\Shilong\\murmur\\03_circor_states\\absent_test_id.csv\"\n",
    "present_test_id_path = r\"D:\\Shilong\\murmur\\03_circor_states\\present_test_id.csv\"\n",
    "csv_path = r\"D:\\Shilong\\murmur\\dataset_all\\training_data.csv\"\n",
    "\n",
    "# get dataset tag from table\n",
    "row_line = csv_reader_row(csv_path, 0)\n",
    "tag_list = []\n",
    "\n",
    "# get index for 'Patient ID' and 'Outcome'\n",
    "tag_list.append(row_line.index(\"Patient ID\"))\n",
    "tag_list.append(row_line.index(\"Murmur\"))\n",
    "tag_list.append(row_line.index(\"Recording locations:\"))\n",
    "\n",
    "absent_test_id = csv_reader_cl(absent_test_id_path, 0)\n",
    "present_test_id = csv_reader_cl(present_test_id_path, 0)\n",
    "id_data = csv_reader_cl(csv_path, tag_list[0])\n",
    "Murmur = csv_reader_cl(csv_path, tag_list[1])\n",
    "Recording_locations = csv_reader_cl(csv_path, tag_list[2])\n",
    "# 所有测试数据的id\n",
    "test_id = absent_test_id+present_test_id\n",
    "patient_target=[]\n",
    "patient_dic={}\n",
    "# print(absent_test_id)\n",
    "for id in test_id:\n",
    "    murmur=Murmur[id_data.index(id)]\n",
    "    if murmur=='present':\n",
    "        patient_target.append(1)\n",
    "    else:\n",
    "        patient_target.append(0)\n",
    "    locations=Recording_locations[id_data.index(id)]\n",
    "    patient_dic[id]=locations\n",
    "\n",
    "# result_dic用来存储分类结果,formate: id_pos: result\n",
    "result_dic={}\n",
    "# patient_result_dic用于保存每个患者每个听诊区的分类结果，formate: id: location1_result,location2_result\n",
    "patient_result_dic={}\n",
    "print(patient_dic)\n",
    "for patient_id,locations in patient_dic.items():\n",
    "    for location in locations.split('+'):\n",
    "        id_loc=patient_id+'_'+location\n",
    "        if  id_loc in result_dic.keys():\n",
    "            if not patient_id in patient_result_dic.keys():\n",
    "                patient_result_dic[patient_id]=result_dic[id_loc]\n",
    "            else:\n",
    "                patient_result_dic[patient_id]+=result_dic[id_loc]\n",
    "        else:\n",
    "            print('[waring]: '+id_loc+' not in result_dic')\n",
    "\n",
    "# 遍历patient_result_dic，计算每个患者的最终分类结果\n",
    "patient_output_dic={}\n",
    "patient_output=[]\n",
    "patient_target=[]\n",
    "for patient_id,result in patient_result_dic.items():\n",
    "    # 做output\n",
    "    if np.mean(result)==0:\n",
    "        patient_output_dic[patient_id]=0\n",
    "        patient_output.append(0)\n",
    "    else:\n",
    "        patient_output_dic[patient_id]=1\n",
    "        patient_output.append(1)\n",
    "    # 做target\n",
    "    if patient_id in absent_test_id:\n",
    "        patient_target.append(0)\n",
    "    elif patient_target in present_test_id:\n",
    "        patient_target.append(1)\n",
    "    else:\n",
    "        print('[waring]: '+patient_id+' not in test_id')\n",
    "        \n",
    "# 计算准确率和混淆矩阵\n",
    "# 计算准确率\n",
    "patient_acc = (np.array(patient_output) == np.array(\n",
    "    patient_target)).sum()/len(patient_target)\n",
    "# 计算混淆矩阵\n",
    "patient_cm = confusion_matrix(patient_target, patient_output)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patient分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.BEATs_def import draw_confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "def draw_confusion_matrix(\n",
    "    cm,\n",
    "    label_name,\n",
    "    title=\"Confusion Matrix\",\n",
    "    pdf_save_path=None,\n",
    "    dpi=600,\n",
    "    epoch=0,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    @param label_true: 真实标签，比如[0,1,2,7,4,5,...]\n",
    "    @param label_pred: 预测标签，比如[0,5,4,2,1,4,...]\n",
    "    @param label_name: 标签名字，比如['cat','dog','flower',...]\n",
    "    @param normlize: 是否设元素为百分比形式\n",
    "    @param title: 图标题\n",
    "    @param pdf_save_path: 是否保存，是则为保存路径pdf_save_path=xxx.png | xxx.pdf | ...等其他plt.savefig支持的保存格式\n",
    "    @param dpi: 保存到文件的分辨率，论文一般要求至少300dpi\n",
    "    @return:\n",
    "\n",
    "    example：\n",
    "            draw_confusion_matrix(label_true=y_gt,\n",
    "                          label_pred=y_pred,\n",
    "                          label_name=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"],\n",
    "                          normlize=True,\n",
    "                          title=\"Confusion Matrix on Fer2013\",\n",
    "                          pdf_save_path=\"Confusion_Matrix_on_Fer2013.png\",\n",
    "                          dpi=300)\n",
    "\n",
    "    \"\"\"\n",
    "    row_sums = np.sum(cm, axis=1)  # 计算每行的和\n",
    "    cm = cm.T\n",
    "    plt.imshow(cm.T, cmap=\"Greens\")\n",
    "    # plt.title(title)\n",
    "    plt.xlabel(\"Predict label\")\n",
    "    plt.ylabel(\"Truth label\")\n",
    "    plt.yticks(range(label_name.__len__()), label_name)\n",
    "    plt.xticks(range(label_name.__len__()), label_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i in range(label_name.__len__()):\n",
    "        for j in range(label_name.__len__()):\n",
    "            # color = (1, 1, 1) if i == j else (0, 0, 0)  # 对角线字体白色，其他黑色\n",
    "            # value = float(format(\"%.4f\" % cm[i, j]))\n",
    "            str_value = \"{}\".format(cm[i, j])\n",
    "            plt.text(\n",
    "                i,\n",
    "                j,\n",
    "                str_value,\n",
    "                verticalalignment=\"center\",\n",
    "                horizontalalignment=\"center\",\n",
    "                # color=color,\n",
    "            )\n",
    "\n",
    "    # plt.show()\n",
    "    if not pdf_save_path is None:\n",
    "        if not os.path.exists(pdf_save_path):\n",
    "            os.makedirs(pdf_save_path)\n",
    "        plt.savefig(\n",
    "            pdf_save_path + \"/epoch\" + str(epoch) + \".png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=dpi,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "patient_cm=np.array([[136,3], [ 5,31]])\n",
    "draw_confusion_matrix(\n",
    "            patient_cm,\n",
    "            [\"Absent\", \"Present\"],\n",
    "            \"epoch428 testacc:95.429%\",\n",
    "            pdf_save_path = r\"./location_cm/\" +str(datetime.now().strftime(\"%Y-%m%d %H%M\")),\n",
    "            epoch = 428\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data=[0.2 0.1]\n",
    "\n",
    "# data=(data-np.mean(data))/(np.max(data)-np.min(data))\n",
    "# data\n",
    "data=np.array(data).reshape(-1,1)\n",
    "# data=data-np.mean(data)\n",
    "data = preprocessing.MinMaxScaler((-1,1)).fit_transform(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "a=[]\n",
    "for i in range(-5,5):\n",
    "    b=[]\n",
    "    b.append(i)\n",
    "    a.append(b)\n",
    "\n",
    "# #print(a)   \n",
    "l=np.array(a)\n",
    "print('l=',l)\n",
    "x = preprocessing.MaxAbsScaler().fit_transform(l)\n",
    "print('x=',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [7, 13, 14, 3],\n",
       " 1: [1, 11, 17, 16],\n",
       " 2: [4, 0, 12, 9],\n",
       " 3: [5, 18, 2, 15],\n",
       " 4: [8, 10, 6]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 五折\n",
    "import numpy as np\n",
    "import random\n",
    "data_absnet =list(range(19))\n",
    "random.shuffle(data)\n",
    "flod5_absent={}\n",
    "for i in range(5):\n",
    "    if i not in flod5_absent:\n",
    "        flod5_absent[i] = []\n",
    "    flod5_absent[i].extend(data[i*4:i*4+4])\n",
    "flod5_absent\n",
    "# present\n",
    "data_absnet =list(range(19))\n",
    "random.shuffle(data)\n",
    "flod5_absent={}\n",
    "flod5_absent[i] = []\n",
    "flod5_absent[i].extend(data[i*4:i*4+4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始序列SIZE:694\n",
      "第0折:size:139, value:[423, 229, 639, 521, 561, 178, 581, 136, 654, 458, 225, 438, 636, 145, 643, 304, 512, 81, 629, 678, 246, 434, 269, 590, 281, 50, 531, 47, 623, 190, 96, 399, 652, 383, 152, 447, 632, 464, 212, 201, 477, 511, 392, 545, 230, 336, 608, 621, 57, 550, 567, 622, 486, 565, 150, 358, 460, 137, 200, 612, 498, 223, 315, 17, 86, 657, 185, 240, 48, 22, 658, 469, 670, 508, 268, 119, 271, 60, 23, 497, 578, 182, 640, 37, 490, 328, 161, 467, 348, 356, 210, 89, 105, 505, 65, 628, 386, 425, 135, 292, 474, 245, 441, 687, 11, 601, 359, 407, 41, 448, 382, 593, 525, 49, 553, 32, 156, 502, 38, 507, 384, 197, 676, 54, 648, 307, 297, 582, 27, 99, 345, 562, 134, 592, 352, 158, 75, 334, 324] \n",
      "第1折:size:139, value:[333, 258, 30, 579, 179, 584, 580, 530, 568, 247, 9, 646, 299, 3, 129, 679, 396, 689, 391, 28, 516, 395, 394, 404, 457, 2, 471, 233, 342, 548, 351, 595, 625, 326, 473, 416, 309, 556, 277, 594, 52, 357, 500, 250, 120, 535, 390, 666, 172, 78, 77, 264, 238, 67, 476, 373, 224, 261, 408, 575, 536, 323, 117, 387, 85, 133, 243, 488, 108, 633, 677, 279, 576, 683, 284, 175, 241, 528, 421, 572, 656, 31, 231, 406, 140, 514, 355, 123, 372, 526, 234, 169, 481, 468, 617, 442, 409, 195, 524, 517, 542, 440, 272, 6, 603, 320, 149, 267, 254, 570, 7, 188, 62, 347, 300, 127, 262, 162, 367, 519, 130, 426, 626, 142, 237, 449, 265, 484, 25, 653, 668, 419, 257, 198, 183, 306, 509, 21, 263] \n",
      "第2折:size:139, value:[661, 577, 369, 174, 624, 51, 194, 388, 205, 244, 563, 389, 214, 412, 532, 596, 276, 213, 503, 222, 534, 101, 33, 36, 614, 125, 112, 144, 485, 69, 221, 288, 115, 610, 495, 691, 496, 138, 462, 365, 620, 160, 615, 587, 301, 103, 12, 456, 283, 26, 291, 114, 541, 630, 651, 141, 24, 68, 559, 302, 202, 56, 255, 465, 432, 61, 5, 443, 380, 337, 660, 59, 90, 422, 126, 450, 148, 116, 506, 219, 94, 540, 415, 341, 551, 455, 180, 451, 564, 204, 34, 228, 170, 330, 107, 20, 655, 371, 232, 607, 665, 235, 688, 437, 139, 66, 472, 522, 106, 681, 649, 100, 605, 445, 446, 647, 344, 208, 285, 377, 329, 266, 619, 398, 675, 15, 124, 102, 164, 436, 435, 664, 310, 627, 128, 487, 673, 168, 598] \n",
      "第3折:size:139, value:[298, 64, 588, 0, 313, 248, 491, 19, 433, 613, 338, 340, 499, 368, 18, 659, 604, 504, 600, 418, 260, 374, 91, 482, 452, 381, 546, 166, 193, 251, 71, 685, 690, 363, 631, 510, 692, 217, 533, 159, 176, 84, 686, 280, 378, 294, 385, 82, 42, 287, 429, 70, 427, 327, 121, 583, 350, 544, 45, 364, 10, 132, 14, 667, 606, 181, 206, 74, 325, 16, 634, 680, 430, 218, 249, 645, 370, 318, 8, 461, 203, 635, 618, 215, 322, 98, 411, 410, 555, 475, 552, 420, 641, 585, 402, 211, 493, 242, 83, 527, 189, 642, 312, 239, 574, 353, 227, 554, 478, 111, 354, 589, 146, 543, 273, 173, 295, 311, 286, 539, 39, 76, 693, 72, 296, 220, 155, 376, 537, 209, 191, 650, 644, 73, 13, 682, 314, 566, 401] \n",
      "第4折:size:138, value:[252, 207, 549, 366, 53, 638, 275, 571, 167, 63, 321, 669, 400, 95, 538, 454, 518, 92, 339, 87, 43, 413, 453, 569, 319, 520, 403, 684, 154, 171, 187, 110, 256, 397, 29, 259, 494, 361, 405, 226, 439, 349, 379, 611, 513, 424, 184, 602, 1, 431, 293, 480, 463, 44, 597, 470, 616, 393, 79, 118, 459, 303, 131, 492, 305, 332, 414, 479, 489, 663, 147, 335, 672, 317, 662, 143, 192, 40, 270, 165, 55, 466, 157, 560, 362, 637, 547, 501, 113, 599, 282, 444, 216, 88, 153, 278, 58, 186, 151, 591, 375, 483, 331, 609, 163, 671, 523, 109, 35, 346, 417, 97, 586, 308, 428, 674, 46, 316, 177, 196, 253, 93, 573, 236, 199, 515, 558, 122, 557, 80, 343, 290, 4, 104, 360, 289, 529, 274] \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def flod_devide(data,flod_num=5):\n",
    "    # 打乱序列\n",
    "    random.shuffle(data)\n",
    "    # 五折\n",
    "    flod5={}\n",
    "    point=[]\n",
    "    for i in range(flod_num):\n",
    "        point.append(i*round(len(data)/flod_num))\n",
    "    # print(point)\n",
    "    # 分割序列 \n",
    "    for i in range(len(point)):\n",
    "        if i<len(point)-1:    \n",
    "            flod5[i] = []\n",
    "            flod5[i].extend(data[point[i]:point[i+1]])\n",
    "        else:\n",
    "            flod5[i] = []\n",
    "            flod5[i].extend(data[point[-1]:])\n",
    "    return flod5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    # # 定义原始序列\n",
    "    sequence = list(range(0,694))\n",
    "    print(\"原始序列SIZE:\"+str(len(sequence)))\n",
    "    flod5=flod_devide(sequence,5)\n",
    "    # # 打乱序列\n",
    "    # random.shuffle(sequence)\n",
    "    # print(\"打乱后的序列:\", sequence)\n",
    "\n",
    "    # # 五折\n",
    "    # flod_num=5\n",
    "    # # present\n",
    "    # point=[]\n",
    "    # for i in range(flod_num):\n",
    "    #     point.append(i*round(len(sequence)/flod_num))\n",
    "    # # print(point)\n",
    "    # flod5={}\n",
    "    # # 分割序列 \n",
    "    # for i in range(len(point)):\n",
    "    #     if i<len(point)-1:    \n",
    "    #         flod5[i] = []\n",
    "    #         flod5[i].extend(sequence[point[i]:point[i+1]])\n",
    "    #     else:\n",
    "    #         flod5[i] = []\n",
    "    #         flod5[i].extend(sequence[point[-1]:])\n",
    "    # print(\"分割后的序列:\")\n",
    "    for k,v in flod5.items():\n",
    "        print(\"第{}折:size:{}, value:{} \".format(k,len(v),v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\n",
      "['absent', 'present', 'reverse0.8', 'reverse0.9', 'reverse1.0', 'reverse1.1', 'reverse1.2', 'time_stretch0.8', 'time_stretch0.9', 'time_stretch1.1', 'time_stretch1.2']\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\absent\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\present\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\reverse0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\reverse0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\reverse1.0\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\reverse1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\reverse1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\time_stretch0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\time_stretch0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\time_stretch1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_0\\time_stretch1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\n",
      "['absent', 'present', 'reverse0.8', 'reverse0.9', 'reverse1.0', 'reverse1.1', 'reverse1.2', 'time_stretch0.8', 'time_stretch0.9', 'time_stretch1.1', 'time_stretch1.2']\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\absent\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\present\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\reverse0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\reverse0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\reverse1.0\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\reverse1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\reverse1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\time_stretch0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\time_stretch0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\time_stretch1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_1\\time_stretch1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\n",
      "['absent', 'present', 'reverse0.8', 'reverse0.9', 'reverse1.0', 'reverse1.1', 'reverse1.2', 'time_stretch0.8', 'time_stretch0.9', 'time_stretch1.1', 'time_stretch1.2']\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\absent\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\present\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\reverse0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\reverse0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\reverse1.0\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\reverse1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\reverse1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\time_stretch0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\time_stretch0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\time_stretch1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_2\\time_stretch1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\n",
      "['absent', 'present', 'reverse0.8', 'reverse0.9', 'reverse1.0', 'reverse1.1', 'reverse1.2', 'time_stretch0.8', 'time_stretch0.9', 'time_stretch1.1', 'time_stretch1.2']\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\absent\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\present\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\reverse0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\reverse0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\reverse1.0\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\reverse1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\reverse1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\time_stretch0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\time_stretch0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\time_stretch1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_3\\time_stretch1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\n",
      "['absent', 'present', 'reverse0.8', 'reverse0.9', 'reverse1.0', 'reverse1.1', 'reverse1.2', 'time_stretch0.8', 'time_stretch0.9', 'time_stretch1.1', 'time_stretch1.2']\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\absent\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\present\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\reverse0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\reverse0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\reverse1.0\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\reverse1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\reverse1.2\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\time_stretch0.8\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\time_stretch0.9\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\time_stretch1.1\n",
      "D:\\Shilong\\murmur\\01_dataset\\05_5fold\\fold_set_4\\time_stretch1.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = r\"D:\\Shilong\\murmur\\01_dataset\\05_5fold\"\n",
    "for k in range(5):    \n",
    "    src_fold_root_path = root_path+r\"\\fold_set_\"+str(k)\n",
    "    print(src_fold_root_path)\n",
    "    a=os.listdir(src_fold_root_path)\n",
    "    print(a)\n",
    "    # for name in a:\n",
    "    #     print(src_fold_root_path+'\\\\'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14241_AV', '14241_MV', '14241_PV', '14241_TV', '44514_AV', '44514_MV', '44514_PV', '44514_TV', '46579_AV', '46579_MV', '46579_PV', '46579_TV', '49572_MV', '49572_PV', '49572_TV', '49691_AV', '49691_MV', '49691_PV', '49691_TV', '49748_MV', '49748_TV', '49850_AV', '49850_MV', '50056_AV', '50056_MV', '50056_PV', '50056_TV', '50099_MV', '50099_PV', '50099_TV', '50129_AV', '50129_MV', '50129_PV', '50129_TV', '50161_MV', '50161_PV', '50161_TV', '50229_AV', '50229_MV', '50229_PV', '50229_TV', '50285_MV', '50289_AV', '50289_MV', '50289_PV', '50289_TV', '50746_AV', '50746_MV', '50746_PV', '50746_TV', '51064_MV', '57700_MV', '57700_PV', '57700_TV', '68204_AV', '68204_MV', '68204_PV', '68204_TV', '68379_AV', '68379_MV', '68379_PV', '68379_TV', '68487_AV', '68487_MV', '68487_PV', '68487_TV', '69120_AV', '69120_MV', '69120_PV', '69120_TV', '83094_MV', '84692_AV', '84692_MV', '84692_PV', '84692_TV', '84695_AV', '84695_MV', '84695_PV', '84695_TV', '84714_AV', '84714_MV', '84714_TV', '84732_AV', '84732_MV', '84751_AV', '84751_MV', '84751_PV', '84751_TV', '84854_AV', '84854_MV', '84854_PV', '84854_TV', '84896_AV', '84896_MV', '84896_PV', '84896_TV', '85002_AV', '85002_MV', '85002_PV', '85002_TV', '85165_AV', '85165_MV', '85207_AV', '85207_MV', '85207_PV', '85207_TV', '85243_AV', '85243_PV', '85259_AV', '85259_MV', '85261_AV', '85261_MV', '85261_PV', '85261_TV', '85306_AV', '85306_MV']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "path_a=r'D:\\Shilong\\murmur\\01_dataset\\01_s1s2\\test\\Present'\n",
    "absent_id_path = r'D:\\Shilong\\murmur\\01_dataset\\01_s1s2\\Present_test_id.csv'\n",
    "a = os.listdir(path_a)\n",
    "print(a)\n",
    "absent_test_id=[]\n",
    "for name in a:\n",
    "    segs=name.split('_')[0]\n",
    "    if segs not in absent_test_id:\n",
    "        absent_test_id.append(segs)\n",
    "pd.DataFrame(data=absent_test_id, index=None).to_csv(\n",
    "    absent_id_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5\n",
      "[0.53939394 0.47878788 0.41818182 0.         0.6        0.66060606\n",
      " 0.72121212 0.78181818 0.84242424 0.9030303  1.        ]\n",
      "16.5\n",
      "[0.53939394 0.47878788 0.41818182 0.         0.         0.06060606\n",
      " 0.12121212 0.18181818 0.24242424 0.3030303  0.4       ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def wav_normalize(data):\n",
    "    \"\"\"归一化\"\"\"\n",
    "    range = np.max(data) - np.min(data)\n",
    "    data = (data-np.min(data))/range\n",
    "    print(range)\n",
    "    return data\n",
    "\n",
    "\n",
    "def wav_normalize_new(data):\n",
    "    rang = np.max(data) - np.min(data)\n",
    "    print(rang)\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i] = (data[i] - np.min(data)) / rang\n",
    "    return data\n",
    "\n",
    "a=np.array([-1,-2,-3,-9.9,0.0,1.0,2.0,3.0,4.0,5.0,6.6])\n",
    "\n",
    "print(wav_normalize(a))\n",
    "print(wav_normalize_new(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7102)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import binary_auprc, binary_auroc, binary_f1_score, binary_confusion_matrix, binary_accuracy, binary_precision, binary_recall\n",
    "input = torch.tensor([0,0,0,1,1,1,1,1,1,1])\n",
    "target = torch.tensor([1, 0, 1, 1,0,0,1,1,1,1])\n",
    "binary_confusion_matrix(input, target, normalize='none')\n",
    "binary_auprc(input, target )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Lubdub\n",
    "def getSpringerPCGFeatures(despiked_data, Fs, audio_segmentation_Fs=50, figures=False):\n",
    "\n",
    "    ########## Homomorphic envelope ##########\n",
    "\n",
    "    # Find the homomorphic envelope\n",
    "    homomorphic_envelope = Homomorphic_Envelope_with_Hilbert(despiked_data, Fs)\n",
    "    # plt.plot(homomorphic_envelope)\n",
    "    # Downsample the envelope:\n",
    "    num_samps = round(np.size(homomorphic_envelope)/Fs*audio_segmentation_Fs)\n",
    "    downsampled_homomorphic_envelope = signal.resample(\n",
    "        homomorphic_envelope, num_samps)\n",
    "    # normalise the envelope:\n",
    "    downsampled_homomorphic_envelope = normalize_signal(\n",
    "        downsampled_homomorphic_envelope)\n",
    "    # plt.plot(downsampled_homomorphic_envelope)\n",
    "    ########## Hilbert Envelope ##########\n",
    "    # Hilbert Envelope\n",
    "    hilbert_envelope = Hilbert_Envelope(despiked_data, Fs, figures=0)\n",
    "    downsampled_hilbert_envelope = signal.resample(hilbert_envelope, num_samps)\n",
    "    downsampled_hilbert_envelope = normalize_signal(\n",
    "        downsampled_hilbert_envelope)\n",
    "    ########## Get power spectral density features ##########\n",
    "    psd = get_PSD_feature_Springer_HMM(despiked_data, Fs, 40, 60)\n",
    "    psd = signal.resample_poly(\n",
    "        psd, len(downsampled_homomorphic_envelope), len(psd))\n",
    "    psd = normalize_signal(psd)\n",
    "    ########## Wavelet features ##########\n",
    "    # if include_wavelet_features:\n",
    "    wavelet_level = 3\n",
    "    wavelet_name = 'db7'\n",
    "    [cA, cD, _, _] = pywt.wavedec(\n",
    "        despiked_data, wavelet_name, mode='zero', level=wavelet_level)\n",
    "    wavelet_feature = signal.resample_poly(\n",
    "        abs(cD), len(downsampled_homomorphic_envelope), len(cD))\n",
    "    wavelet_feature = normalize_signal(wavelet_feature)\n",
    "    ########## Wavelet features ##########\n",
    "    # if include_wavelet_features:\n",
    "    PCG_Features = [downsampled_homomorphic_envelope,\n",
    "                    downsampled_hilbert_envelope, psd, wavelet_feature]\n",
    "\n",
    "    return PCG_Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamatong特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Shilong\\murmur\\00_Code\\LM\\beats1\\BEATs\\BEATs_try.ipynb 单元格 77\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y136sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y\u001b[39m=\u001b[39mlibrosa\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mnormalize(y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y136sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m magnitude \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(librosa\u001b[39m.\u001b[39mstft(y,win_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y136sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m Gam\u001b[39m=\u001b[39mgammatone_filter_bank\u001b[39m.\u001b[39;49mdot(magnitude)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y136sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m LogGamSpec \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mpower_to_db(Gam,ref\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mmax)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from spafe.fbanks.gammatone_fbanks import gammatone_filter_banks\n",
    "File_name = r'D:\\Shilong\\murmur\\01_dataset\\07_newnorm\\Present\\9979\\9979_AV.wav'\n",
    "y,sr=librosa.load(File_name,sr=4000)\n",
    "gammatone_filter_bank = gammatone_filter_banks(nfilts=64, nfft=512, fs=sr, low_freq=100, high_freq=None,  order=4)\n",
    "y=librosa.util.normalize(y)\n",
    "magnitude = np.abs(librosa.stft(y,win_length=512))**2\n",
    "Gam=gammatone_filter_bank.dot(magnitude)\n",
    "LogGamSpec = librosa.power_to_db(Gam,ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from patient_information import get_height, get_weight, load_patient_data, get_pregnancy_status, get_age, compare_strings, get_sex\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "from scipy import signal\n",
    "# from python_speech_features import logfbank\n",
    "from spafe.features.gfcc import erb_spectrogram\n",
    "from spafe.utils.preprocessing import SlidingWindow\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def Log_GF(data_directory):\n",
    "    x, fs = librosa.load(os.path.join(data_directory, f), sr=4000)\n",
    "    # energy = Energy(x,fs,100,50)\n",
    "    gSpec, gfreqs = erb_spectrogram(x,fs=fs,pre_emph=0,pre_emph_coeff=0.97,window=SlidingWindow(0.025, 0.0125, \"hamming\"),nfilts=64,nfft=512,low_freq=25,high_freq=2000)\n",
    "    fbank_feat = gSpec.T\n",
    "    # fbank_feat = fbank_feat*energy\n",
    "    fbank_feat = np.log(fbank_feat)\n",
    "    fbank_feat = (fbank_feat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6831)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "p = torch.tensor([0.2057, 0.8667, 1.0000])\n",
    "r = torch.tensor([1.0000, 0.7222, 0.0000])\n",
    "# p[0]*r[0]+p[1]*r[1]-p[0]*r[1]\n",
    "-torch.sum((r[1:] -r[:-1])*p[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Shilong\\murmur\\00_Code\\LM\\beats1\\BEATs\\BEATs_try.ipynb 单元格 80\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y142sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m threshold, indices \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msort(descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y142sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m mask \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(threshold\u001b[39m.\u001b[39mdiff(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m, [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Shilong/murmur/00_Code/LM/beats1/BEATs/BEATs_try.ipynb#Y142sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m num_tp \u001b[39m=\u001b[39m (target[indices] \u001b[39m==\u001b[39m pos_label)\u001b[39m.\u001b[39mcumsum(\u001b[39m0\u001b[39m)[mask]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "pos_label=1\n",
    "target = torch.tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
    "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
    "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
    "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0])\n",
    "input=torch.tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0])\n",
    "threshold, indices = input.sort(descending=True)\n",
    "mask = F.pad(threshold.diff(dim=0) != 0, [0, 1], value=1.0)\n",
    "num_tp = (target[indices] == pos_label).cumsum(0)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0991)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_ecg.models.loss import FocalLoss\n",
    "import torch\n",
    "a=torch.tensor([1,0,1,1,0])\n",
    "aa=torch.sigmoid(a)\n",
    "b=torch.tensor([0,0,0,0,1])\n",
    "l=FocalLoss(class_weight=torch.tensor(0.25),  reduction='mean')\n",
    "l(aa,b.float())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_Shilong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
