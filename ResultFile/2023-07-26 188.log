[2023-07-26 18:08:08 - INFO:] # batch_size = 64
[2023-07-26 18:08:08 - INFO:] # learning_rate = 0.0001
[2023-07-26 18:08:08 - INFO:] # num_epochs = 30
[2023-07-26 18:08:08 - INFO:] # padding_size = 3500
[2023-07-26 18:08:08 - INFO:] # criterion = CrossEntropyLoss()
[2023-07-26 18:08:08 - INFO:] # optimizer = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.98)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
[2023-07-26 18:08:08 - INFO:] -------------------------------
[2023-07-26 18:08:46 - INFO:] epoch: 1/30
[2023-07-26 18:08:46 - INFO:] learning_rate: 0.0001
[2023-07-26 18:08:46 - INFO:] train_acc: 65.6731%, train_loss: 0.0102
[2023-07-26 18:08:46 - INFO:] test_acc: 69.8049%, test_loss: 0.0097
[2023-07-26 18:08:46 - INFO:] max_train_acc: 65.6731%
[2023-07-26 18:08:46 - INFO:] max_test_acc: 69.8049%
[2023-07-26 18:08:46 - INFO:] max_lr: 0.0001, min_lr: 0.0001
[2023-07-26 18:08:46 - INFO:] ======================================
