[2023-0728 0956 - INFO] <<< BEATs_iter3_plus_AS2M - 2 fc layers>>>
[2023-0728 0956 - INFO] # trainset_size = 17010
[2023-0728 0956 - INFO] # testset_size = 22970
[2023-0728 0956 - INFO] # train_a/p = 8505/8505
[2023-0728 0956 - INFO] # test_a/p = 20866/2050
[2023-0728 0956 - INFO] # batch_size = 64
[2023-0728 0956 - INFO] # learning_rate = 0.0001
[2023-0728 0956 - INFO] # num_epochs = 100
[2023-0728 0956 - INFO] # padding_size = 3500
[2023-0728 0956 - INFO] # criterion = CrossEntropyLoss()
[2023-0728 0956 - INFO] # scheduler = None
[2023-0728 0956 - INFO] # optimizer = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.98)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
[2023-0728 0956 - INFO] -------------------------------
[2023-0728 0957 - INFO] epoch: 1/100
[2023-0728 0957 - INFO] learning_rate: 0.0001
[2023-0728 0957 - INFO] train_acc: 66.432%, train_loss: 0.0105
[2023-0728 0957 - INFO] test_acc: 66.613%, test_loss: 0.0102
[2023-0728 0957 - INFO] max_train_acc: 66.432%
[2023-0728 0957 - INFO] max_test_acc: 66.613%
[2023-0728 0957 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 0957 - INFO] ======================================
[2023-0728 0958 - INFO] epoch: 2/100
[2023-0728 0958 - INFO] learning_rate: 0.0001
[2023-0728 0958 - INFO] train_acc: 69.424%, train_loss: 0.0100
[2023-0728 0958 - INFO] test_acc: 66.657%, test_loss: 0.0099
[2023-0728 0958 - INFO] max_train_acc: 69.424%
[2023-0728 0958 - INFO] max_test_acc: 66.657%
[2023-0728 0958 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 0958 - INFO] ======================================
[2023-0728 0959 - INFO] epoch: 3/100
[2023-0728 0959 - INFO] learning_rate: 0.0001
[2023-0728 0959 - INFO] train_acc: 69.383%, train_loss: 0.0097
[2023-0728 0959 - INFO] test_acc: 66.678%, test_loss: 0.0096
[2023-0728 0959 - INFO] max_train_acc: 69.424%
[2023-0728 0959 - INFO] max_test_acc: 66.678%
[2023-0728 0959 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 0959 - INFO] ======================================
[2023-0728 1000 - INFO] epoch: 4/100
[2023-0728 1000 - INFO] learning_rate: 0.0001
[2023-0728 1000 - INFO] train_acc: 69.342%, train_loss: 0.0095
[2023-0728 1000 - INFO] test_acc: 66.717%, test_loss: 0.0094
[2023-0728 1000 - INFO] max_train_acc: 69.424%
[2023-0728 1000 - INFO] max_test_acc: 66.717%
[2023-0728 1000 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1000 - INFO] ======================================
[2023-0728 1001 - INFO] epoch: 5/100
[2023-0728 1001 - INFO] learning_rate: 0.0001
[2023-0728 1001 - INFO] train_acc: 69.565%, train_loss: 0.0094
[2023-0728 1001 - INFO] test_acc: 66.778%, test_loss: 0.0094
[2023-0728 1001 - INFO] max_train_acc: 69.565%
[2023-0728 1001 - INFO] max_test_acc: 66.778%
[2023-0728 1001 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1001 - INFO] ======================================
[2023-0728 1002 - INFO] epoch: 6/100
[2023-0728 1002 - INFO] learning_rate: 0.0001
[2023-0728 1002 - INFO] train_acc: 69.671%, train_loss: 0.0093
[2023-0728 1002 - INFO] test_acc: 66.887%, test_loss: 0.0093
[2023-0728 1002 - INFO] max_train_acc: 69.671%
[2023-0728 1002 - INFO] max_test_acc: 66.887%
[2023-0728 1002 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1002 - INFO] ======================================
[2023-0728 1003 - INFO] epoch: 7/100
[2023-0728 1003 - INFO] learning_rate: 0.0001
[2023-0728 1003 - INFO] train_acc: 69.847%, train_loss: 0.0093
[2023-0728 1003 - INFO] test_acc: 66.992%, test_loss: 0.0093
[2023-0728 1003 - INFO] max_train_acc: 69.847%
[2023-0728 1003 - INFO] max_test_acc: 66.992%
[2023-0728 1003 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1003 - INFO] ======================================
[2023-0728 1004 - INFO] epoch: 8/100
[2023-0728 1004 - INFO] learning_rate: 0.0001
[2023-0728 1004 - INFO] train_acc: 70.076%, train_loss: 0.0092
[2023-0728 1004 - INFO] test_acc: 67.427%, test_loss: 0.0091
[2023-0728 1004 - INFO] max_train_acc: 70.076%
[2023-0728 1004 - INFO] max_test_acc: 67.427%
[2023-0728 1004 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1004 - INFO] ======================================
[2023-0728 1005 - INFO] epoch: 9/100
[2023-0728 1005 - INFO] learning_rate: 0.0001
[2023-0728 1005 - INFO] train_acc: 70.447%, train_loss: 0.0092
[2023-0728 1005 - INFO] test_acc: 67.767%, test_loss: 0.0091
[2023-0728 1005 - INFO] max_train_acc: 70.447%
[2023-0728 1005 - INFO] max_test_acc: 67.767%
[2023-0728 1005 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1005 - INFO] ======================================
[2023-0728 1006 - INFO] epoch: 10/100
[2023-0728 1006 - INFO] learning_rate: 0.0001
[2023-0728 1006 - INFO] train_acc: 70.817%, train_loss: 0.0091
[2023-0728 1006 - INFO] test_acc: 68.659%, test_loss: 0.0090
[2023-0728 1006 - INFO] max_train_acc: 70.817%
[2023-0728 1006 - INFO] max_test_acc: 68.659%
[2023-0728 1006 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1006 - INFO] ======================================
[2023-0728 1007 - INFO] epoch: 11/100
[2023-0728 1007 - INFO] learning_rate: 0.0001
[2023-0728 1007 - INFO] train_acc: 71.158%, train_loss: 0.0091
[2023-0728 1007 - INFO] test_acc: 70.592%, test_loss: 0.0089
[2023-0728 1007 - INFO] max_train_acc: 71.158%
[2023-0728 1007 - INFO] max_test_acc: 70.592%
[2023-0728 1007 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1007 - INFO] ======================================
[2023-0728 1008 - INFO] epoch: 12/100
[2023-0728 1008 - INFO] learning_rate: 0.0001
[2023-0728 1008 - INFO] train_acc: 71.487%, train_loss: 0.0091
[2023-0728 1008 - INFO] test_acc: 71.846%, test_loss: 0.0088
[2023-0728 1008 - INFO] max_train_acc: 71.487%
[2023-0728 1008 - INFO] max_test_acc: 71.846%
[2023-0728 1008 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1008 - INFO] ======================================
[2023-0728 1010 - INFO] epoch: 13/100
[2023-0728 1010 - INFO] learning_rate: 0.0001
[2023-0728 1010 - INFO] train_acc: 72.087%, train_loss: 0.0090
[2023-0728 1010 - INFO] test_acc: 73.927%, test_loss: 0.0087
[2023-0728 1010 - INFO] max_train_acc: 72.087%
[2023-0728 1010 - INFO] max_test_acc: 73.927%
[2023-0728 1010 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1010 - INFO] ======================================
[2023-0728 1011 - INFO] epoch: 14/100
[2023-0728 1011 - INFO] learning_rate: 0.0001
[2023-0728 1011 - INFO] train_acc: 72.222%, train_loss: 0.0090
[2023-0728 1011 - INFO] test_acc: 75.098%, test_loss: 0.0086
[2023-0728 1011 - INFO] max_train_acc: 72.222%
[2023-0728 1011 - INFO] max_test_acc: 75.098%
[2023-0728 1011 - INFO] max_lr: 0.0001, min_lr: 0.0001
[2023-0728 1011 - INFO] ======================================
