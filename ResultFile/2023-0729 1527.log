[2023-0729 1527 - INFO] <<< BEATs_iter3_plus_AS2M >>> - 1 fc layer
[2023-0729 1527 - INFO] # trainset_size = 17010
[2023-0729 1527 - INFO] # testset_size = 22970
[2023-0729 1527 - INFO] # train_a/p = 8505/8505
[2023-0729 1527 - INFO] # test_a/p = 20920/2050
[2023-0729 1527 - INFO] # batch_size = 64
[2023-0729 1527 - INFO] # learning_rate = 0.0005
[2023-0729 1527 - INFO] # num_epochs = 100
[2023-0729 1527 - INFO] # padding_size = 3500
[2023-0729 1527 - INFO] # criterion = CrossEntropyLoss()
[2023-0729 1527 - INFO] # scheduler = None
[2023-0729 1527 - INFO] # optimizer = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.01
)
[2023-0729 1527 - INFO] -------------------------------
[2023-0729 1528 - INFO] epoch: 1/100
[2023-0729 1528 - INFO] learning_rate: 0.0005
[2023-0729 1528 - INFO] train_acc: 69.300%, train_loss: 0.0096
[2023-0729 1528 - INFO] test_acc: 67.575%, test_loss: 0.0092
[2023-0729 1528 - INFO] max_train_acc: 69.300%
[2023-0729 1528 - INFO] max_test_acc: 67.575%
[2023-0729 1528 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1528 - INFO] ======================================
[2023-0729 1529 - INFO] epoch: 2/100
[2023-0729 1529 - INFO] learning_rate: 0.0005
[2023-0729 1529 - INFO] train_acc: 71.123%, train_loss: 0.0091
[2023-0729 1529 - INFO] test_acc: 76.530%, test_loss: 0.0085
[2023-0729 1529 - INFO] max_train_acc: 71.123%
[2023-0729 1529 - INFO] max_test_acc: 76.530%
[2023-0729 1529 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1529 - INFO] ======================================
[2023-0729 1530 - INFO] epoch: 3/100
[2023-0729 1530 - INFO] learning_rate: 0.0005
[2023-0729 1530 - INFO] train_acc: 73.104%, train_loss: 0.0089
[2023-0729 1530 - INFO] test_acc: 79.121%, test_loss: 0.0082
[2023-0729 1530 - INFO] max_train_acc: 73.104%
[2023-0729 1530 - INFO] max_test_acc: 79.121%
[2023-0729 1530 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1530 - INFO] ======================================
[2023-0729 1532 - INFO] epoch: 4/100
[2023-0729 1532 - INFO] learning_rate: 0.0005
[2023-0729 1532 - INFO] train_acc: 74.039%, train_loss: 0.0088
[2023-0729 1532 - INFO] test_acc: 81.572%, test_loss: 0.0078
[2023-0729 1532 - INFO] max_train_acc: 74.039%
[2023-0729 1532 - INFO] max_test_acc: 81.572%
[2023-0729 1532 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1532 - INFO] ======================================
[2023-0729 1533 - INFO] epoch: 5/100
[2023-0729 1533 - INFO] learning_rate: 0.0005
[2023-0729 1533 - INFO] train_acc: 74.915%, train_loss: 0.0087
[2023-0729 1533 - INFO] test_acc: 80.205%, test_loss: 0.0079
[2023-0729 1533 - INFO] max_train_acc: 74.915%
[2023-0729 1533 - INFO] max_test_acc: 81.572%
[2023-0729 1533 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1533 - INFO] ======================================
[2023-0729 1534 - INFO] epoch: 6/100
[2023-0729 1534 - INFO] learning_rate: 0.0005
[2023-0729 1534 - INFO] train_acc: 74.927%, train_loss: 0.0087
[2023-0729 1534 - INFO] test_acc: 80.022%, test_loss: 0.0079
[2023-0729 1534 - INFO] max_train_acc: 74.927%
[2023-0729 1534 - INFO] max_test_acc: 81.572%
[2023-0729 1534 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1534 - INFO] ======================================
[2023-0729 1535 - INFO] epoch: 7/100
[2023-0729 1535 - INFO] learning_rate: 0.0005
[2023-0729 1535 - INFO] train_acc: 75.126%, train_loss: 0.0086
[2023-0729 1535 - INFO] test_acc: 79.983%, test_loss: 0.0079
[2023-0729 1535 - INFO] max_train_acc: 75.126%
[2023-0729 1535 - INFO] max_test_acc: 81.572%
[2023-0729 1535 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1535 - INFO] ======================================
[2023-0729 1536 - INFO] epoch: 8/100
[2023-0729 1536 - INFO] learning_rate: 0.0005
[2023-0729 1536 - INFO] train_acc: 75.350%, train_loss: 0.0086
[2023-0729 1536 - INFO] test_acc: 80.170%, test_loss: 0.0079
[2023-0729 1536 - INFO] max_train_acc: 75.350%
[2023-0729 1536 - INFO] max_test_acc: 81.572%
[2023-0729 1536 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1536 - INFO] ======================================
[2023-0729 1537 - INFO] epoch: 9/100
[2023-0729 1537 - INFO] learning_rate: 0.0005
[2023-0729 1537 - INFO] train_acc: 75.556%, train_loss: 0.0085
[2023-0729 1537 - INFO] test_acc: 80.135%, test_loss: 0.0079
[2023-0729 1537 - INFO] max_train_acc: 75.556%
[2023-0729 1537 - INFO] max_test_acc: 81.572%
[2023-0729 1537 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1537 - INFO] ======================================
[2023-0729 1538 - INFO] epoch: 10/100
[2023-0729 1538 - INFO] learning_rate: 0.0005
[2023-0729 1538 - INFO] train_acc: 75.197%, train_loss: 0.0086
[2023-0729 1538 - INFO] test_acc: 80.788%, test_loss: 0.0078
[2023-0729 1538 - INFO] max_train_acc: 75.556%
[2023-0729 1538 - INFO] max_test_acc: 81.572%
[2023-0729 1538 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1538 - INFO] ======================================
[2023-0729 1539 - INFO] epoch: 11/100
[2023-0729 1539 - INFO] learning_rate: 0.0005
[2023-0729 1539 - INFO] train_acc: 75.567%, train_loss: 0.0085
[2023-0729 1539 - INFO] test_acc: 81.593%, test_loss: 0.0077
[2023-0729 1539 - INFO] max_train_acc: 75.567%
[2023-0729 1539 - INFO] max_test_acc: 81.593%
[2023-0729 1539 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0729 1539 - INFO] ======================================
