[2023-0804 1331 - INFO] <<< BEATs_iter3_plus_AS2M - 2 fc layer >>> 
[2023-0804 1331 - INFO] Add FrequencyMasking and TimeMasking
[2023-0804 1331 - INFO] Add time_stretch 0.8 and time_stretch 1.2
[2023-0804 1331 - INFO] # trainset_size = 17010
[2023-0804 1331 - INFO] # testset_size = 27070
[2023-0804 1331 - INFO] # train_a/p = 8505/8505
[2023-0804 1331 - INFO] # test_a/p = 20920/6150
[2023-0804 1331 - INFO] # batch_size = 128
[2023-0804 1331 - INFO] # learning_rate = 0.0005
[2023-0804 1331 - INFO] # weight_decay = 0.01
[2023-0804 1331 - INFO] # num_epochs = 100
[2023-0804 1331 - INFO] # padding_size = 3500
[2023-0804 1331 - INFO] # loss_fn = CE
[2023-0804 1331 - INFO] # scheduler = None
[2023-0804 1331 - INFO] # optimizer = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.01
)
[2023-0804 1331 - INFO] -------------------------------------
[2023-0804 1332 - INFO] epoch: 1/100
[2023-0804 1332 - INFO] learning_rate: 0.0005
[2023-0804 1332 - INFO] train_acc: 25.162%, train_loss: nan
[2023-0804 1332 - INFO] test_acc: 61.426%, test_loss: 0.0053
[2023-0804 1332 - INFO] max_train_acc: 25.162%
[2023-0804 1332 - INFO] max_test_acc: 61.426%
[2023-0804 1332 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1332 - INFO] ======================================
[2023-0804 1334 - INFO] epoch: 2/100
[2023-0804 1334 - INFO] learning_rate: 0.0005
[2023-0804 1334 - INFO] train_acc: 25.109%, train_loss: nan
[2023-0804 1334 - INFO] test_acc: 61.334%, test_loss: 0.0053
[2023-0804 1334 - INFO] max_train_acc: 25.162%
[2023-0804 1334 - INFO] max_test_acc: 61.426%
[2023-0804 1334 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1334 - INFO] ======================================
[2023-0804 1335 - INFO] epoch: 3/100
[2023-0804 1335 - INFO] learning_rate: 0.0005
[2023-0804 1335 - INFO] train_acc: 25.726%, train_loss: nan
[2023-0804 1335 - INFO] test_acc: 61.345%, test_loss: 0.0053
[2023-0804 1335 - INFO] max_train_acc: 25.726%
[2023-0804 1335 - INFO] max_test_acc: 61.426%
[2023-0804 1335 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1335 - INFO] ======================================
[2023-0804 1336 - INFO] epoch: 4/100
[2023-0804 1336 - INFO] learning_rate: 0.0005
[2023-0804 1336 - INFO] train_acc: 25.420%, train_loss: nan
[2023-0804 1336 - INFO] test_acc: 61.326%, test_loss: 0.0053
[2023-0804 1336 - INFO] max_train_acc: 25.726%
[2023-0804 1336 - INFO] max_test_acc: 61.426%
[2023-0804 1336 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1336 - INFO] ======================================
[2023-0804 1338 - INFO] epoch: 5/100
[2023-0804 1338 - INFO] learning_rate: 0.0005
[2023-0804 1338 - INFO] train_acc: 24.803%, train_loss: nan
[2023-0804 1338 - INFO] test_acc: 61.426%, test_loss: 0.0053
[2023-0804 1338 - INFO] max_train_acc: 25.726%
[2023-0804 1338 - INFO] max_test_acc: 61.426%
[2023-0804 1338 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1338 - INFO] ======================================
[2023-0804 1339 - INFO] epoch: 6/100
[2023-0804 1339 - INFO] learning_rate: 0.0005
[2023-0804 1339 - INFO] train_acc: 24.856%, train_loss: nan
[2023-0804 1339 - INFO] test_acc: 61.426%, test_loss: 0.0053
[2023-0804 1339 - INFO] max_train_acc: 25.726%
[2023-0804 1339 - INFO] max_test_acc: 61.426%
[2023-0804 1339 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1339 - INFO] ======================================
[2023-0804 1341 - INFO] epoch: 7/100
[2023-0804 1341 - INFO] learning_rate: 0.0005
[2023-0804 1341 - INFO] train_acc: 24.956%, train_loss: nan
[2023-0804 1341 - INFO] test_acc: 61.322%, test_loss: 0.0053
[2023-0804 1341 - INFO] max_train_acc: 25.726%
[2023-0804 1341 - INFO] max_test_acc: 61.426%
[2023-0804 1341 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1341 - INFO] ======================================
[2023-0804 1342 - INFO] epoch: 8/100
[2023-0804 1342 - INFO] learning_rate: 0.0005
[2023-0804 1342 - INFO] train_acc: 24.615%, train_loss: nan
[2023-0804 1342 - INFO] test_acc: 61.156%, test_loss: 0.0053
[2023-0804 1342 - INFO] max_train_acc: 25.726%
[2023-0804 1342 - INFO] max_test_acc: 61.426%
[2023-0804 1342 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1342 - INFO] ======================================
[2023-0804 1344 - INFO] epoch: 9/100
[2023-0804 1344 - INFO] learning_rate: 0.0005
[2023-0804 1344 - INFO] train_acc: 24.838%, train_loss: nan
[2023-0804 1344 - INFO] test_acc: 61.204%, test_loss: 0.0053
[2023-0804 1344 - INFO] max_train_acc: 25.726%
[2023-0804 1344 - INFO] max_test_acc: 61.426%
[2023-0804 1344 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1344 - INFO] ======================================
[2023-0804 1345 - INFO] epoch: 10/100
[2023-0804 1345 - INFO] learning_rate: 0.0005
[2023-0804 1345 - INFO] train_acc: 24.109%, train_loss: nan
[2023-0804 1345 - INFO] test_acc: 61.555%, test_loss: 0.0053
[2023-0804 1345 - INFO] max_train_acc: 25.726%
[2023-0804 1345 - INFO] max_test_acc: 61.555%
[2023-0804 1345 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1345 - INFO] ======================================
[2023-0804 1347 - INFO] epoch: 11/100
[2023-0804 1347 - INFO] learning_rate: 0.0005
[2023-0804 1347 - INFO] train_acc: 25.344%, train_loss: nan
[2023-0804 1347 - INFO] test_acc: 61.430%, test_loss: 0.0053
[2023-0804 1347 - INFO] max_train_acc: 25.726%
[2023-0804 1347 - INFO] max_test_acc: 61.555%
[2023-0804 1347 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1347 - INFO] ======================================
[2023-0804 1348 - INFO] epoch: 12/100
[2023-0804 1348 - INFO] learning_rate: 0.0005
[2023-0804 1348 - INFO] train_acc: 25.009%, train_loss: nan
[2023-0804 1348 - INFO] test_acc: 61.178%, test_loss: 0.0053
[2023-0804 1348 - INFO] max_train_acc: 25.726%
[2023-0804 1348 - INFO] max_test_acc: 61.555%
[2023-0804 1348 - INFO] max_lr: 0.0005, min_lr: 0.0005
[2023-0804 1348 - INFO] ======================================
