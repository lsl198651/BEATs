[2023-07-26 14:44:14 - INFO:] # batch_size = 128
[2023-07-26 14:44:14 - INFO:] # learning_rate = 0.005
[2023-07-26 14:44:14 - INFO:] # num_epochs = 200
[2023-07-26 14:44:14 - INFO:] # padding_size = 3500
[2023-07-26 14:44:14 - INFO:] # criterion = CrossEntropyLoss()
[2023-07-26 14:44:14 - INFO:] # scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x00000242E5BA64C0>
[2023-07-26 14:44:14 - INFO:] # optimizer = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.98)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.005
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
[2023-07-26 14:44:14 - INFO:] -------------------------------
[2023-07-26 14:47:06 - INFO:] epoch: 1/200
[2023-07-26 14:47:06 - INFO:] learning_rate: 0.0000
[2023-07-26 14:47:06 - INFO:] train_acc: 86.8866%, train_loss: 0.0052
[2023-07-26 14:47:06 - INFO:] test_acc: 88.3544%, test_loss: 0.0052
[2023-07-26 14:47:06 - INFO:] max_train_acc: 86.8866%
[2023-07-26 14:47:06 - INFO:] max_test_acc: 88.3544%
[2023-07-26 14:47:06 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 14:47:06 - INFO:] ======================================
[2023-07-26 14:49:59 - INFO:] epoch: 2/200
[2023-07-26 14:49:59 - INFO:] learning_rate: 0.0000
[2023-07-26 14:49:59 - INFO:] train_acc: 88.5601%, train_loss: 0.0052
[2023-07-26 14:49:59 - INFO:] test_acc: 90.7967%, test_loss: 0.0051
[2023-07-26 14:49:59 - INFO:] max_train_acc: 88.5601%
[2023-07-26 14:49:59 - INFO:] max_test_acc: 90.7967%
[2023-07-26 14:49:59 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 14:49:59 - INFO:] ======================================
[2023-07-26 14:52:46 - INFO:] epoch: 3/200
[2023-07-26 14:52:46 - INFO:] learning_rate: 0.0000
[2023-07-26 14:52:46 - INFO:] train_acc: 90.4711%, train_loss: 0.0051
[2023-07-26 14:52:46 - INFO:] test_acc: 90.8533%, test_loss: 0.0050
[2023-07-26 14:52:46 - INFO:] max_train_acc: 90.4711%
[2023-07-26 14:52:46 - INFO:] max_test_acc: 90.8533%
[2023-07-26 14:52:46 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 14:52:46 - INFO:] ======================================
[2023-07-26 14:55:31 - INFO:] epoch: 4/200
[2023-07-26 14:55:31 - INFO:] learning_rate: 0.0000
[2023-07-26 14:55:31 - INFO:] train_acc: 90.8649%, train_loss: 0.0050
[2023-07-26 14:55:31 - INFO:] test_acc: 90.8228%, test_loss: 0.0049
[2023-07-26 14:55:31 - INFO:] max_train_acc: 90.8649%
[2023-07-26 14:55:31 - INFO:] max_test_acc: 90.8533%
[2023-07-26 14:55:31 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 14:55:31 - INFO:] ======================================
[2023-07-26 14:58:17 - INFO:] epoch: 5/200
[2023-07-26 14:58:17 - INFO:] learning_rate: 0.0000
[2023-07-26 14:58:17 - INFO:] train_acc: 90.9072%, train_loss: 0.0049
[2023-07-26 14:58:17 - INFO:] test_acc: 90.8359%, test_loss: 0.0048
[2023-07-26 14:58:17 - INFO:] max_train_acc: 90.9072%
[2023-07-26 14:58:17 - INFO:] max_test_acc: 90.8533%
[2023-07-26 14:58:17 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 14:58:17 - INFO:] ======================================
[2023-07-26 15:01:09 - INFO:] epoch: 6/200
[2023-07-26 15:01:09 - INFO:] learning_rate: 0.0000
[2023-07-26 15:01:09 - INFO:] train_acc: 90.9051%, train_loss: 0.0047
[2023-07-26 15:01:09 - INFO:] test_acc: 90.8402%, test_loss: 0.0046
[2023-07-26 15:01:09 - INFO:] max_train_acc: 90.9072%
[2023-07-26 15:01:09 - INFO:] max_test_acc: 90.8533%
[2023-07-26 15:01:09 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 15:01:09 - INFO:] ======================================
[2023-07-26 15:03:57 - INFO:] epoch: 7/200
[2023-07-26 15:03:57 - INFO:] learning_rate: 0.0000
[2023-07-26 15:03:57 - INFO:] train_acc: 90.9114%, train_loss: 0.0046
[2023-07-26 15:03:57 - INFO:] test_acc: 90.8533%, test_loss: 0.0044
[2023-07-26 15:03:57 - INFO:] max_train_acc: 90.9114%
[2023-07-26 15:03:57 - INFO:] max_test_acc: 90.8533%
[2023-07-26 15:03:57 - INFO:] max_lr: 0.0000, min_lr: 0.0000
[2023-07-26 15:03:57 - INFO:] ======================================
